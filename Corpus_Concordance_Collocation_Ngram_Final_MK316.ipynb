{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPQpdqqtBgD6V8//xDNO7KB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Corpus (코퍼스/말뭉치)\n",
        "- 자연어 연구에 사용되고 형태소 분석을 사전에 시행하여 분석에 정확성을 기한다. 코퍼스 분석은 단어수 계산, n-gram 빈도수와 범워, 키값, 연어(collocation), 의존합자식별(예: 동사-직접목적어 조합) 및 의존합자식별 빈도-범위-연결강도 분석을 한다.\n",
        "\n",
        "- 위의 분석을 위해서는 아래 패키지를 설치한다\n",
        "-- corpus-toolkit : tokenization(토큰화) & lemmatization(표준형태화) 포함\n",
        "- 참조: Kyle 의 코드에는 Spacy 패키지(tagging(단어에 문법범주 연결) & parsing(구분자를 사용하여 문자열을 구성 요소로 분석))를 포함 하였으나, 여기에서는 Spacy 패키지를 따로 설치하지 않고, nltk에서 사용하는 코드를 사용할 예정.  \n"
      ],
      "metadata": {
        "id": "M7afPLwh-bRv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBz4z96Z-YV5",
        "outputId": "db7919dc-19a7-4f48-c07b-6e991cb00b5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: corpus-toolkit in /usr/local/lib/python3.7/dist-packages (0.32)\n"
          ]
        }
      ],
      "source": [
        "pip install corpus-toolkit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-processing (사전처리)\n",
        "- 코퍼스 분석을 하려면 분석하려는 텍스트의 단어를 토큰화하여 분석할 수 있는 형태로 준비해 놓아야 한다. i) tokenize 함수를 사용하여 토큰을 리스트화 하여 준비해 놓을 수도 있고, ii) tag 함수를 사용하여 토큰에 문법범주(품사)를 연결해서 준비해 놓을 수도 있다.  \n",
        "-- [코드문법1] corpus_toolkit패키지에서 corpus_tools모듈을 ct로 줄여서 불러들여라.\n",
        "-- [코드문법2] [파일 올리고 읽기] ct모듈의 ldcorpus( )함수에 brwon_single 데이터 폴더를 인자로 넣어, 그 결과를 brown_corp변수에 할당한다.\n",
        "-- [코드문법3] [단어 토큰화] ct모듈의 tokenize( )함수 인자로 바로 위 코드 변수 brown_corp를 넣어, 그 결과를 tok_corp변수에 할당한다.\n",
        "-- [코드문법4] [토큰화한 단어 빈도수] ct모듈의 frequency( )함수 인자로 바로 위 코드 변수 tok_corp를 넣어, 그 결과를 brown_freq변수에 할당한다.\n",
        "-- [코드문법5] ct모듈의 head( )에 brown_freq변수를 첫 째 인자로, 빈도수가 높은 것 10개를 둘 째 인자로 사용해서, 상위 10개 토큰의 빈도수를 출력한다."
      ],
      "metadata": {
        "id": "bO38yLp90864"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab 작업 디렉토리에서 폴더를 생성하여 다른 곳에서 다운로드 받은 단/복수의 파일을 옮겨 넣는 방법\n",
        "- 코랩에 기본으로 깔려 있는 sample_data 에 커서를 대면 활성화되면서 우측에 세로 점 3 개가 나타난다. 거기를 클릭하면 선택사항 -> new folder를 클릭한다.\n",
        "- new folder를 drag해서 sample_data 쪽으로 끌어오면 같은 레벨로 폴더가 이동한다.\n",
        "- new folder에 커서를 대면 활성화되면서 우측에 세로 점 3 개가 나타난다.Rename folder 클릭해서 파일명을 새이름으로 바꾼다. 예)brown_single\n",
        "- 해당 폴더에 커서를 대면 활성화되면서 우측에 세로 점 3 개가 나타난다.Upload 클릭해서 본인 컴퓨터에 다운로드 받은 복수의 파일을 모두 업로드 한다."
      ],
      "metadata": {
        "id": "IpAwLbK32qeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from corpus_toolkit import corpus_tools as ct\n",
        "brown_corp = ct.ldcorpus(\"brown_single\") #load and read corpus: ca~cd from brown_single original folder for class use\n",
        "tok_corp = ct.tokenize(brown_corp) #tokenize corpus - by default this lemmatizes as well\n",
        "brown_freq = ct.frequency(tok_corp) #creates a frequency dictionary\n",
        "##note that range can be calculated instead of frequency using the argument calc = \"range\"\n",
        "ct.head(brown_freq, hits = 10) #print top 10 items"
      ],
      "metadata": {
        "id": "17lsll3VIzu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 명령어 차곡히 쌓기 (Nest)\n",
        "위에서 파일을 로드해서 읽고 -> 토큰화 하고 -> 토큰의 빈도수를 계산하는 명령어를 사용하여 코드로 작성했다. 아래 코드와 같이 모듈.함수( )인 명령어를 차곡차곡 쌓아서 같은 목적을 달성할 수 있고 이를 추천한다."
      ],
      "metadata": {
        "id": "R-98Zbi6j8qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brown_freq = ct.frequency(ct.tokenize(ct.ldcorpus(\"brown_single\")))\n",
        "ct.head(brown_freq, hits = 10)"
      ],
      "metadata": {
        "id": "zxrWoaKbj-Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conc_results1 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False),[\"run\",\"ran\",\"running\",\"runs\"],nhits = 10)\n",
        "for x in conc_results1:\n",
        "\tprint(x)"
      ],
      "metadata": {
        "id": "_z3M-DivngQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conc_results2 = ct.concord(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False),[\"run\",\"ran\",\"running\",\"runs\"],collocates = [\"suddenly\", 'just'], nhits = 10)\n",
        "for x in conc_results2:\n",
        "\tprint(x)"
      ],
      "metadata": {
        "id": "AZRzHbwUnjgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collocates = ct.collocator(ct.tokenize(ct.ldcorpus(\"brown_single\")),\"go\",stat = \"MI\")\n",
        "#stat options include: \"MI\", \"T\", \"freq\", \"left\", and \"right\"\n",
        "\n",
        "ct.head(collocates, hits = 10)"
      ],
      "metadata": {
        "id": "TWCovRiTtJtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##N-grams\n",
        "N-grams are contiguous sequences of n words. The tokenize() function can be used to create an n-gram version of a corpus by employing the ngram argument. By default, words in an n-gram are separated by two underscores \"__\""
      ],
      "metadata": {
        "id": "AC3sbfm_uRO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trigramfreq = ct.frequency(ct.tokenize(ct.ldcorpus(\"brown_single\"),lemma = False, ngram = 3))\n",
        "ct.head(trigramfreq, hits = 10)"
      ],
      "metadata": {
        "id": "Ll9zKBO4uAo0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}