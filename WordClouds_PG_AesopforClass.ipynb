{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPevtYjtFMdMvUZUmpLnPBT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms624atyale/Python_Basics/blob/main/WordClouds_PG_AesopforClass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🐹 🐾 [Text Corpus <font size='1.8'>코퍼스/말뭉치</font>](https://en.wikipedia.org/wiki/Text_corpus)\n",
        "- In linguistics, a corpus (plural corpora) or text corpus is a language resource consisting of a large and structured set of texts (nowadays usually electronically stored and processed). In corpus linguistics, they are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory.\n",
        "\n",
        "- The **corpus-toolkit** package grew out of courses in corpus linguistics and learner corpus research. The toolkit attempts to balance simplicity of use, broad application, and scalability. Common corpus analyses such as the <font color = 'red'>_calculation of word and n-gram frequency and range, keyness, and collocation_</font> are included. In addition, more advanced analyses such as the identification of <font color = 'red'>_dependency bigrams (e.g., verb-direct object combinations) and their frequency, range, and strength of association_</font>  are also included.(https://pypi.org/project/corpus-toolkit/)\n",
        "\n",
        "Some conditions should be fulfilled if you want to conduct corpus-related analysis.\n",
        "\n",
        ">1. Read and write a file using an operating system package.\n",
        ">2. 🆘 import the **[os](https://docs.python.org/3/library/os.html)** module.\n",
        "\n",
        "\n",
        ">3. Text files you want to analyze (e.g., url(uniform resource locator) with html document, text files under the Files dicrectory of Google Colab).\n",
        ">4. Text ➡️ Words: **Tokenization**\n",
        ">5. Words with the conjugation, inflection, derivation process ↔️ Words sorted by grouping inflected or variant forms of the same word (i.e., **lemmatization**)\n",
        ">6. POS (part of speech (e.g., word-grammatical category pairs))\n",
        ">7. 🆘 Install **corpus-toolkit** and **nltk**(natural language tool kit) packages.\n",
        "\n"
      ],
      "metadata": {
        "id": "0Bgbjzrdk0GX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfS5hTQgkzSM"
      },
      "outputs": [],
      "source": [
        "#@markdown 📌 Download the os module\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Make a new working directory as \"txtdata\". 📎 <Module name: os> <function: mkdir>\n",
        "\n",
        "os.mkdir(\"txtdata\")"
      ],
      "metadata": {
        "id": "jIjVMmz7pf2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Download the corpus-toolkit package\n",
        "!pip install corpus-toolkit"
      ],
      "metadata": {
        "id": "fHhAr36fphbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🆘 **curl**: Curl is a command-line tool for making HTTP requests. It can be used to download content from a URL.\n",
        "\n",
        "\"**curl** \" + **url** + \" > **txtdata.txt**\": This part of the command is constructing a string that includes the URL (url) and specifies the output redirection to a file named \"txtdata.txt\". The > symbol is used for output redirection in the shell.\n",
        "\n",
        "So, in simple terms, the os.system(\"curl \" + url + \" > txtdata.txt\") line is downloading the content from the specified URL using the curl command and saving it to a file named \"txtdata.txt\"."
      ],
      "metadata": {
        "id": "W0QLJi2lpudo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "\n",
        "#@markdown 📌 Open a txt file. <Use a set of duble quotation marks \"\" and assign the url address as the _url_ variable>\n",
        "url=\"https://raw.githubusercontent.com/ms624atyale/Temp_Data/main/PG_Aesop_NoQuotesAllText.txt\"\n",
        "#When you copy and paste, i) go to the repository of your/someone else's github, ii) click on a txt file of your interest, iii) click on <raw> icon around at the top right hand corner, and iii) copy & paste the url staring with \"https://raw.githubusercontent.com/...\"\n",
        "\n",
        "\n",
        "os.system(\"curl \" + url + \" > txtdata.txt\") #This moves the whole text of the url to the txtdata folder.\n",
        "\n",
        "file = open(\"txtdata.txt\")\n",
        "text = file.read().replace(\"\\n\", \" \") #Replace line with a space.\n",
        "file.close() #Close the file you have been working on.\n",
        "\n",
        "#@markdown 📎 When you see txtdata.txt under the Files directory, move it under the txtdata folder you've created by drag & drop."
      ],
      "metadata": {
        "id": "tSgEo377phR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🆘 **Drag your txtdata.txt file to the txtdata folder.**"
      ],
      "metadata": {
        "id": "nuFD6QYEp1iJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 i) Tokenize your text and ii) Get frequency.\n",
        "\n",
        "# Import the corpus_tools module from the corpus_toolkit library\n",
        "from corpus_toolkit import corpus_tools as ct\n",
        "mydata = ct.ldcorpus(\"txtdata\") #load and read text files under 'txtdata' directory\n",
        "tok_corp = ct.tokenize(mydata) #tokenize corpus - by default this lemmatizes as well\n",
        "for token in tok_corp:\n",
        "  print(token)\n",
        "\n",
        "count = len(token)\n",
        "print(count)\n",
        "\n",
        "#💣💊You cannot get tokens by simply using print(tok_corp). You should use FOR Statement!"
      ],
      "metadata": {
        "id": "_PAJd7osphIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#🆘 ct is an abbreviation for corpus-tools module.\n",
        "\n",
        "mydata = ct.frequency(mydata)\n",
        "#The frequency() function is being used to generate/create a frequency dictionary from the mydata corpus."
      ],
      "metadata": {
        "id": "gO6Yc3H2phAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Tagging (i.e., associating each token with a grammatical category (e.g., mountain - N) )\n",
        "ct.write_corpus(\"tagged_txt\",ct.tag(ct.ldcorpus(\"txtdata\")))"
      ],
      "metadata": {
        "id": "fJvWbRk-pg2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Get frequency of your tagged tokens. 'hits=10' means you want to get the top 10 words.\n",
        "\n",
        "tagged_freq = ct.frequency(ct.reload(\"tagged_txt\"))\n",
        "ct.head(tagged_freq, hits = 10)"
      ],
      "metadata": {
        "id": "jBUamalipgnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 💡 Now, let's save tagged data as a dataframe and get word clouds!"
      ],
      "metadata": {
        "id": "YqEJ9E-qqCz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌  Tagged data is in a dictionary format (e.g., {key:value}).\n",
        "type(tagged_freq)"
      ],
      "metadata": {
        "id": "gCzpQ4IhpjnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Import the pandas package so as to handle dataframe.\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "JznVVIzFpj-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Generate a dateframe with tagged words (e.g., word_POS) and their frequencies.\n",
        "\n",
        "# Assuming 'tagged_freq' is a previously defined dictionary\n",
        "data_dict = tagged_freq\n",
        "# Get the items (key-value pairs) from the dictionary\n",
        "data_items = data_dict.items()\n",
        "# Convert the items to a list\n",
        "data_list = list(data_items)\n",
        "print(data_list)"
      ],
      "metadata": {
        "id": "QaiIecahpkVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#🆘  This code cell is part of the above code cell (#[12]). For an educational purpose, it is separated on purpose.\n",
        "\n",
        "# Create a DataFrame from the list of items\n",
        "df = pd.DataFrame(data_list)\n",
        "# Rename the columns of the DataFrame\n",
        "df.columns = [\"Tagged\",\"Freq\"]\n",
        "# Print the resulting DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "id": "ZmyzLn9tplYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 💡 Splitting tagged columns into Words and POS <font size = '2.3'> part of speech (i.e., grammatical categories)\n",
        "  - e.g.,\n",
        "              column          column 1.    column2\n",
        "          yesterday_ADP ➡️   yesterday       ADP\n",
        "          rain_NOUN             rain         NOUN\n",
        "          yellow_ADJ           yellow.       ADJ"
      ],
      "metadata": {
        "id": "pVPt85JiqN4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Codelines to get tagged columns split into words and POS\n",
        "#🆘 df is an abbreviation for dataframe in Pandas. It is very frequently used.\n",
        "\n",
        "# Extract 'Tagged' column from DataFrame\n",
        "tagged = df[\"Tagged\"]\n",
        "# Initialize empty lists for 'pos' and 'word'\n",
        "pos = []\n",
        "word = []\n",
        "\n",
        "# Loop through each element in 'tagged'\n",
        "for i in range(0, len(tagged)):\n",
        "  # Get the tagged word from the 'Tagged' column\n",
        "  w = tagged[i]\n",
        "  # Split the tagged word into word and POS using underscore as separator\n",
        "  ws = w.split(\"_\")\n",
        "   # Append the word to the 'word' list\n",
        "  word.append(ws[0])\n",
        "  # Append the POS to the 'pos' list\n",
        "  pos.append(ws[1])\n",
        "\n",
        "# Print length of 'tagged', first 10 elements of 'word' and 'pos'\n",
        "print(len(tagged))\n",
        "print(word[:10])\n",
        "print(pos[:10])"
      ],
      "metadata": {
        "id": "oDHt7BCAplv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[✏️Gneral description for the following cell]: This code adds \"POS\" and \"Word\" columns to the DataFrame, rearranges the column order, sorts the DataFrame by \"POS\" and \"Freq\" in descending order, prints the total number of rows, and displays the first few rows of the sorted DataFrame."
      ],
      "metadata": {
        "id": "o2zO_H0AqWtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Add new columns to the dataframe.\n",
        "\n",
        "# Add 'POS' and 'Word' columns to the DataFrame\n",
        "df[\"POS\"] = pos\n",
        "df[\"Word\"] = word\n",
        "\n",
        "# Rearranging column order (remove Tagged column)\n",
        "cols = [\"POS\",\"Word\",\"Freq\"]\n",
        "df = df[cols]\n",
        "\n",
        "# Sort the DataFrame by 'POS' and 'Freq' in descending order\n",
        "df = df.sort_values(by=['POS', 'Freq'], ascending = False)\n",
        "# Print the total number of rows in the DataFrame\n",
        "print(\"Total rows: \", len(df))\n",
        "# Display the first few rows of the sorted DataFrame\n",
        "df.head()"
      ],
      "metadata": {
        "id": "hXiQ5-wypmBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##⛅  Creating wordclouds by POS\n",
        ">1. Below, \"wc\" takes text, not list. So we make word-list-by-POS into a text using 'join'.\n",
        ">2. In addition, the joined text should include words according to their frequency. (e.g., if \"before\" occurs 5 times, then the text should include \"before before before before before\".\n",
        "\n",
        "[✏️Gneral description for the following cell]: This code filters the DataFrame to include only rows where the 'POS' is \"VERB,\" converts the 'Freq' and 'Word' columns to lists, repeats words based on their frequencies using NumPy, joins the repeated words into a single string, and prints the results."
      ],
      "metadata": {
        "id": "aFJnJdNmqcd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Select POS = VERB\n",
        "\n",
        "# Filter rows in the DataFrame where 'POS' is \"VERB\"\n",
        "df1 = df[df[\"POS\"] == \"VERB\"]\n",
        "# Print the number of rows in the filtered DataFrame\n",
        "print(len(df1))\n",
        "\n",
        "\n",
        "# Convert 'Freq' and 'Word' columns to lists; # Print the number of elements in the 'freq1' list\n",
        "freq1 = list(df1[\"Freq\"]); print(len(freq1))\n",
        "txt1 = list(df1[\"Word\"]); print(len(txt1))\n",
        "\n",
        "# Repeat words by frequency using NumPy\n",
        "import numpy as np\n",
        "# x = np.array(txt1)\n",
        "y = np.repeat(txt1, freq1, axis=0)\n",
        "y = list(y)\n",
        "\n",
        " # Print the length of the concatenated string 'txt2'\n",
        "txt2 = ' '.join(y); print(len(txt2))\n",
        "print(txt2)"
      ],
      "metadata": {
        "id": "bhmlqZrjqOzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 High frequency POS\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wc = WordCloud().generate(str(txt2))\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "izlKKmO_qPLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⛅ ⛅The following codes will provide you POS options such as NOUN, ADJ, ADV."
      ],
      "metadata": {
        "id": "6wVJIP4PqlwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✏️ In natural language processing, stop words are commonly used words (e.g., \"the,\" \"and,\" \"is\") that are often filtered out because they don't contribute much to the meaning of a text. Here's an example list of English stop words:\n",
        "\n",
        "stopwords = [ 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\",'wouldn', \"wouldn't\" ]\n",
        "\n",
        "You can customize or extend this list based on your specific needs and the requirements of your natural language processing task. Keep in mind that the appropriateness of stop words may vary depending on the specific context and analysis you are conducting."
      ],
      "metadata": {
        "id": "1fLapdgLrBpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📎 Select POS you want."
      ],
      "metadata": {
        "id": "Rvtxy8FqrFTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Wordcloud by POS:\n",
        "\n",
        "pos = \"VERB\" #@param = [\"VERB\",\"NOUN\",\"ADJ\",\"ADV\"]\n",
        "df1 = df[df[\"POS\"] == pos]; print(len(df1))\n",
        "\n",
        "# as list\n",
        "freq1 = list(df1[\"Freq\"])\n",
        "txt1 = list(df1[\"Word\"])\n",
        "\n",
        "# Repeat words by Freq\n",
        "import numpy as np\n",
        "# x = np.array(txt1)\n",
        "y = np.repeat(txt1, freq1, axis=0)\n",
        "y = list(y)\n",
        "\n",
        "txt2 = ' '.join(y)\n",
        "\n",
        "print(df[df[\"POS\"] == pos])\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Collocations = False (otherwise, Wordcloud takes 'light light' as a collocation and add it on the wordcloud )\n",
        "wc = WordCloud(collocations = False).generate(str(txt2))\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "xOu2m3EYqPfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Wordcloud by POS:\n",
        "\n",
        "pos = \"NOUN\" #@param = [\"VERB\",\"NOUN\",\"ADJ\",\"ADV\"]\n",
        "df1 = df[df[\"POS\"] == pos]; print(len(df1))\n",
        "\n",
        "# as list\n",
        "freq1 = list(df1[\"Freq\"])\n",
        "txt1 = list(df1[\"Word\"])\n",
        "\n",
        "# Repeat words by Freq\n",
        "import numpy as np\n",
        "# x = np.array(txt1)\n",
        "y = np.repeat(txt1, freq1, axis=0)\n",
        "y = list(y)\n",
        "\n",
        "txt2 = ' '.join(y)\n",
        "\n",
        "print(df[df[\"POS\"] == pos])\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Collocations = False (otherwise, Wordcloud takes 'light light' as a collocation and add it on the wordcloud )\n",
        "wc = WordCloud(collocations = False).generate(str(txt2))\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "Gn8TO5yJqP3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Wordcloud by POS:\n",
        "\n",
        "pos = \"ADJ\" #@param = [\"VERB\",\"NOUN\",\"ADJ\",\"ADV\"]\n",
        "df1 = df[df[\"POS\"] == pos]; print(len(df1))\n",
        "\n",
        "# as list\n",
        "freq1 = list(df1[\"Freq\"])\n",
        "txt1 = list(df1[\"Word\"])\n",
        "\n",
        "# Repeat words by Freq\n",
        "import numpy as np\n",
        "# x = np.array(txt1)\n",
        "y = np.repeat(txt1, freq1, axis=0)\n",
        "y = list(y)\n",
        "\n",
        "txt2 = ' '.join(y)\n",
        "\n",
        "print(df[df[\"POS\"] == pos])\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Collocations = False (otherwise, Wordcloud takes 'light light' as a collocation and add it on the wordcloud )\n",
        "wc = WordCloud(collocations = False).generate(str(txt2))\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "iMqOUq_yqQTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Wordcloud by POS:\n",
        "\n",
        "pos = \"ADV\" #@param = [\"VERB\",\"NOUN\",\"ADJ\",\"ADV\"]\n",
        "df1 = df[df[\"POS\"] == pos]; print(len(df1))\n",
        "\n",
        "# as list\n",
        "freq1 = list(df1[\"Freq\"])\n",
        "txt1 = list(df1[\"Word\"])\n",
        "\n",
        "# Repeat words by Freq\n",
        "import numpy as np\n",
        "# x = np.array(txt1)\n",
        "y = np.repeat(txt1, freq1, axis=0)\n",
        "y = list(y)\n",
        "\n",
        "txt2 = ' '.join(y)\n",
        "\n",
        "print(df[df[\"POS\"] == pos])\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Collocations = False (otherwise, Wordcloud takes 'light light' as a collocation and add it on the wordcloud )\n",
        "wc = WordCloud(collocations = False).generate(str(txt2))\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "_RXn1953qQqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ⛅ ⛅ I don't like the background in black. I want it WHITE!!! ⛄⚡"
      ],
      "metadata": {
        "id": "Xh3qiSyCq88q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Exclude short words from your word cloud.\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "#stopwords.add('us')\n",
        "len(stopwords)\n",
        "spltxt = text.split()"
      ],
      "metadata": {
        "id": "5TQQyWIbqRQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Wordcloud by POS:\n",
        "\n",
        "#\"#@param\" is a special syntax used in Colab notebooks to create interactive forms.\n",
        "pos = \"VERB\" #@param = [\"VERB\",\"NOUN\",\"ADJ\",\"ADV\"]\n",
        "# Filter DataFrame to include only rows where 'POS' is equal to the chosen part of speech\n",
        "df1 = df[df[\"POS\"] == pos]\n",
        "# Print the number of rows in the filtered DataFrame\n",
        "print(len(df1))\n",
        "\n",
        "\n",
        "# Convert 'Freq' and 'Word' columns to lists\n",
        "freq1 = list(df1[\"Freq\"])\n",
        "txt1 = list(df1[\"Word\"])\n",
        "\n",
        "# Repeat words by frequency using NumPy\n",
        "import numpy as np\n",
        "# x = np.array(txt1)\n",
        "y = np.repeat(txt1, freq1, axis=0)\n",
        "spltxt = list(y)\n",
        "\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(stopwords=stopwords,relative_scaling = 0.2, random_state=3, collocations = False,\n",
        "                      max_words=2000, background_color='white').generate(' '.join(spltxt))\n",
        "\n",
        "# Plot the word cloud\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Save the word cloud as an image file\n",
        "wordcloud.to_file('wordcloud_title.png')\n"
      ],
      "metadata": {
        "id": "lau9Ysycq3S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Wordcloud by POS:\n",
        "\n",
        "pos = \"NOUN\" #@param = [\"VERB\",\"NOUN\",\"ADJ\",\"ADV\"]\n",
        "df1 = df[df[\"POS\"] == pos]; print(len(df1))\n",
        "\n",
        "# as list\n",
        "freq1 = list(df1[\"Freq\"])\n",
        "txt1 = list(df1[\"Word\"])\n",
        "\n",
        "# Repeat words by Freq\n",
        "import numpy as np\n",
        "# x = np.array(txt1)\n",
        "y = np.repeat(txt1, freq1, axis=0)\n",
        "spltxt = list(y)\n",
        "\n",
        "wordcloud = WordCloud(stopwords=stopwords,relative_scaling = 0.2, random_state=3, collocations = False,\n",
        "                      max_words=2000, background_color='white').generate(' '.join(spltxt))\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "wordcloud.to_file('wordcloud_title.png')"
      ],
      "metadata": {
        "id": "inqqM5Q-q3tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Wordcloud by POS:\n",
        "\n",
        "pos = \"ADJ\" #@param = [\"VERB\",\"NOUN\",\"ADJ\",\"ADV\"]\n",
        "df1 = df[df[\"POS\"] == pos]; print(len(df1))\n",
        "\n",
        "# as list\n",
        "freq1 = list(df1[\"Freq\"])\n",
        "txt1 = list(df1[\"Word\"])\n",
        "\n",
        "# Repeat words by Freq\n",
        "import numpy as np\n",
        "# x = np.array(txt1)\n",
        "y = np.repeat(txt1, freq1, axis=0)\n",
        "spltxt = list(y)\n",
        "\n",
        "wordcloud = WordCloud(stopwords=stopwords,relative_scaling = 0.2, random_state=3, collocations = False,\n",
        "                      max_words=2000, background_color='white').generate(' '.join(spltxt))\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "wordcloud.to_file('wordcloud_title.png')"
      ],
      "metadata": {
        "id": "UtDyJkRzq4Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 📌 Wordcloud by POS:\n",
        "\n",
        "pos = \"ADV\" #@param = [\"VERB\",\"NOUN\",\"ADJ\",\"ADV\"]\n",
        "df1 = df[df[\"POS\"] == pos]; print(len(df1))\n",
        "\n",
        "# as list\n",
        "freq1 = list(df1[\"Freq\"])\n",
        "txt1 = list(df1[\"Word\"])\n",
        "\n",
        "# Repeat words by Freq\n",
        "import numpy as np\n",
        "# x = np.array(txt1)\n",
        "y = np.repeat(txt1, freq1, axis=0)\n",
        "spltxt = list(y)\n",
        "\n",
        "wordcloud = WordCloud(stopwords=stopwords,relative_scaling = 0.2, random_state=3, collocations = False,\n",
        "                      max_words=2000, background_color='white').generate(' '.join(spltxt))\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "wordcloud.to_file('wordcloud_title.png')"
      ],
      "metadata": {
        "id": "8GdMqIAwq4aJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}