{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOR6s4eqy9PMwqlo5rv6Llg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms624atyale/Python_Basics/blob/main/28_Tokenization_VariousWays.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color = 'red'> **Corpus data (e.g., crawling) should be preprocessed before further analysis by means of Cleaning(ì •ì œ), Tokenization(í† í°í™”), Normalization(ì •ê·œí™”)**. \n",
        "* [English Tokenization](https://wikidocs.net/21698)\n",
        "\n",
        "## Cleaning\n",
        "*êµ¬ë‘ì (punctuation (e.g., \".\", \",\", \"?\", \"!\", \";\", \":\")ì„ ì§€ìš°ê¸°\n",
        "\n",
        "*íŠ¹ìˆ˜ë¬¸ì ëª¨ë‘ ì§€ìš°ê¸° \n",
        "\n",
        "\n",
        "## Tokenization \n",
        "\n",
        "* Tokenization: ì£¼ì–´ì§„ ì½”í¼ìŠ¤(corpus)ì—ì„œ í† í°(token)ì´ë¼ ë¶ˆë¦¬ëŠ” ë‹¨ìœ„ (e.g., word, phrase, strings with meaning)ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…\n",
        "* í† í°ì˜ ë‹¨ìœ„ê°€ ìƒí™©ì— ë”°ë¼ ë‹¤ë¥´ì§€ë§Œ, ë³´í†µ ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„ë¡œ í† í°ì„ ì •ì˜í•©ë‹ˆë‹¤. \n",
        "* NLTK, KoNLPYë¥¼ í†µí•´ ì‹¤ìŠµì„ ì§„í–‰í•˜ë©° í† í°í™”\n",
        "\n",
        "Simplest tokenization: êµ¬ë‘ì  ì§€ìš´ í›„ ë„ì–´ì“°ê¸°(whitespace)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜ë¼ë‚´ê¸° ì œì™¸\n"
      ],
      "metadata": {
        "id": "gb1sh28mTSox"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "qWcgp3AzSyi4"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Hereâ€™s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. \\\n",
        "The ones who see things differently â€” theyâ€™re not fond of rules. \\\n",
        "I wanted to pay with a twenty-dolloar bill; however, she couldnâ€™t get cash. \\\n",
        "I like Brownâ€™s East back pack,\\\n",
        "but she doesnâ€™t.'"
      ],
      "metadata": {
        "id": "4odDoSnPS18k"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QovcT0MjS7gp",
        "outputId": "f4327565-7023-4a02-a8fe-3746ad357d5f"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hereâ€™s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. The ones who see things differently â€” theyâ€™re not fond of rules. I wanted to pay with a twenty-dolloar bill; however, she couldnâ€™t get cash. I like Brownâ€™s East back pack,but she doesnâ€™t.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/sample_data/Exercise/text_symbol_sample.txt','rt')\n",
        "file.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "gbFvGofSTFDR",
        "outputId": "be3479a1-543e-411c-c911-b0c117086c92"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hereâ€™s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes.\\nThe ones who see things differently â€” theyâ€™re not fond of rules. \\nI wanted to pay with a twenty-dolloar bill; \\nhowever, but she couldnâ€™t get cash. \\nI like Brownâ€™s East back pack,\\nbut she doesnâ€™t.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/sample_data/Exercise/text_symbol_sample.txt','rt')\n",
        "file.read().replace(\"\\n\", \" \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "afJEPVK-0zZl",
        "outputId": "8e35cf06-c645-4df6-e670-ed479ebde1c3"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hereâ€™s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. The ones who see things differently â€” theyâ€™re not fond of rules.  I wanted to pay with a twenty-dolloar bill;  however, but she couldnâ€™t get cash.  I like Brownâ€™s East back pack, but she doesnâ€™t.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simplest way to tokenize a text into units."
      ],
      "metadata": {
        "id": "sTnGhW7YrT_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/sample_data/Exercise/text_symbol_sample.txt','rt')\n",
        "obj = file.read().replace(\"\\n\", \" \")\n",
        "obj.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWBNRkayhJeF",
        "outputId": "179d1e5b-8d39-4b81-ddec-1a4134597d3f"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hereâ€™s',\n",
              " 'to',\n",
              " 'the',\n",
              " 'crazy',\n",
              " 'ones,',\n",
              " 'the',\n",
              " 'misfits,',\n",
              " 'the',\n",
              " 'rebels,',\n",
              " 'the',\n",
              " 'troublemakers,',\n",
              " 'the',\n",
              " 'round',\n",
              " 'pegs',\n",
              " 'in',\n",
              " 'the',\n",
              " 'square',\n",
              " 'holes.',\n",
              " 'The',\n",
              " 'ones',\n",
              " 'who',\n",
              " 'see',\n",
              " 'things',\n",
              " 'differently',\n",
              " 'â€”',\n",
              " 'theyâ€™re',\n",
              " 'not',\n",
              " 'fond',\n",
              " 'of',\n",
              " 'rules.',\n",
              " 'I',\n",
              " 'wanted',\n",
              " 'to',\n",
              " 'pay',\n",
              " 'with',\n",
              " 'a',\n",
              " 'twenty-dolloar',\n",
              " 'bill;',\n",
              " 'however,',\n",
              " 'but',\n",
              " 'she',\n",
              " 'couldnâ€™t',\n",
              " 'get',\n",
              " 'cash.',\n",
              " 'I',\n",
              " 'like',\n",
              " 'Brownâ€™s',\n",
              " 'East',\n",
              " 'back',\n",
              " 'pack,',\n",
              " 'but',\n",
              " 'she',\n",
              " 'doesnâ€™t.']"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ¹ nltk.tokenize.**word_tokenize()**\n",
        "\n",
        "* <font color = 'sky blue'> Punctuations as well as symbols such as a phrase-linking hiphen\"-\" and an apostrophe \"'\"(perfect tense, possessive) are tokenized.\n",
        "* <font color = 'sky blue'> cf., A within-word hiphen belongs to a word (e.g., 'ten-dollar'), not being tokenized on its own ."
      ],
      "metadata": {
        "id": "A03RIU6gnLZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "\n",
        "print('Tokenizing Word and Punctuation:',word_tokenize(obj)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9Pfw0_Thhka",
        "outputId": "b67a2967-611f-4d83-b25f-91728c2a055e"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing Word and Punctuation: ['Here', 'â€™', 's', 'to', 'the', 'crazy', 'ones', ',', 'the', 'misfits', ',', 'the', 'rebels', ',', 'the', 'troublemakers', ',', 'the', 'round', 'pegs', 'in', 'the', 'square', 'holes', '.', 'The', 'ones', 'who', 'see', 'things', 'differently', 'â€”', 'they', 'â€™', 're', 'not', 'fond', 'of', 'rules', '.', 'I', 'wanted', 'to', 'pay', 'with', 'a', 'twenty-dolloar', 'bill', ';', 'however', ',', 'but', 'she', 'couldn', 'â€™', 't', 'get', 'cash', '.', 'I', 'like', 'Brown', 'â€™', 's', 'East', 'back', 'pack', ',', 'but', 'she', 'doesn', 'â€™', 't', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Advanced for a print codeline.\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "result1 = word_tokenize(obj)\n",
        "\n",
        "print('Tokenizing Word and Punctuation: %s' %result1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1NIz986mFTA",
        "outputId": "c99f1a3a-cc16-4c5a-ec74-23875b82e218"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing Word and Punctuation: ['Here', 'â€™', 's', 'to', 'the', 'crazy', 'ones', ',', 'the', 'misfits', ',', 'the', 'rebels', ',', 'the', 'troublemakers', ',', 'the', 'round', 'pegs', 'in', 'the', 'square', 'holes', '.', 'The', 'ones', 'who', 'see', 'things', 'differently', 'â€”', 'they', 'â€™', 're', 'not', 'fond', 'of', 'rules', '.', 'I', 'wanted', 'to', 'pay', 'with', 'a', 'twenty-dolloar', 'bill', ';', 'however', ',', 'but', 'she', 'couldn', 'â€™', 't', 'get', 'cash', '.', 'I', 'like', 'Brown', 'â€™', 's', 'East', 'back', 'pack', ',', 'but', 'she', 'doesn', 'â€™', 't', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ¹ nltk.tokenize.**WordPunctTokenizer()**\n",
        "\n",
        "* <font color = 'sky blue'> Punctuations (\".\", \",\", \".\", \";\"), a phrase-linking hypthen (\"-\"), a word-linking hyphen (\"-\"), and an apostrophe (\"'\") (perfect tense, possessive, negation)) are tokenized as a unit across the board.\n",
        "\n",
        "  * <font color = 'pink'> Wrong command: >>WordPunctTokenizer(txt) or WordPunctTokenizer.tokenize(txt): Error message: WordPunctTokenizer.__init__() takes 1 positional argument but 2 were given."
      ],
      "metadata": {
        "id": "lOu9-q_Cn01a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "print('Tokenizing Words and Punctuations:', WordPunctTokenizer().tokenize(obj)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYAWzvgbhiwd",
        "outputId": "2740aeb6-b8db-494a-c0be-4e5cb747ca8d"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing Words and Punctuations: ['Here', 'â€™', 's', 'to', 'the', 'crazy', 'ones', ',', 'the', 'misfits', ',', 'the', 'rebels', ',', 'the', 'troublemakers', ',', 'the', 'round', 'pegs', 'in', 'the', 'square', 'holes', '.', 'The', 'ones', 'who', 'see', 'things', 'differently', 'â€”', 'they', 'â€™', 're', 'not', 'fond', 'of', 'rules', '.', 'I', 'wanted', 'to', 'pay', 'with', 'a', 'twenty', '-', 'dolloar', 'bill', ';', 'however', ',', 'but', 'she', 'couldn', 'â€™', 't', 'get', 'cash', '.', 'I', 'like', 'Brown', 'â€™', 's', 'East', 'back', 'pack', ',', 'but', 'she', 'doesn', 'â€™', 't', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "result2 = WordPunctTokenizer().tokenize(obj)\n",
        "\n",
        "print('Tokenizing Words and Punctuations: %s' %result2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kfxxv5cImhFa",
        "outputId": "eab5b638-5015-41f8-c980-3385808cc55f"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing Words and Punctuations: ['Here', 'â€™', 's', 'to', 'the', 'crazy', 'ones', ',', 'the', 'misfits', ',', 'the', 'rebels', ',', 'the', 'troublemakers', ',', 'the', 'round', 'pegs', 'in', 'the', 'square', 'holes', '.', 'The', 'ones', 'who', 'see', 'things', 'differently', 'â€”', 'they', 'â€™', 're', 'not', 'fond', 'of', 'rules', '.', 'I', 'wanted', 'to', 'pay', 'with', 'a', 'twenty', '-', 'dolloar', 'bill', ';', 'however', ',', 'but', 'she', 'couldn', 'â€™', 't', 'get', 'cash', '.', 'I', 'like', 'Brown', 'â€™', 's', 'East', 'back', 'pack', ',', 'but', 'she', 'doesn', 'â€™', 't', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ¹ tensorflow.keras.preprocessing.text.**text_to_word_sequence()**\n",
        "\n",
        "* <font color = 'sky blue'> Small letters across the board\n",
        "* An apostrophe (\"'\" for Perfect tense (e.g., \"I've\"), possessive (e.g., \"John's\"), negation (e.g., 'doesn't)) is part of a token.\n",
        "* A phrase-linking hyphen (-) are tokenized. \n",
        "* **All punctuations and a word-linking phyphen are deleted**. \n"
      ],
      "metadata": {
        "id": "boMBjWuFptSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "print('Tokenizing Words after Cleaning Punctuations:', text_to_word_sequence(obj))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aowUE2sTrAi6",
        "outputId": "a8171414-5888-478d-d95e-9131ea072c3c"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing Words after Cleaning Punctuations: ['hereâ€™s', 'to', 'the', 'crazy', 'ones', 'the', 'misfits', 'the', 'rebels', 'the', 'troublemakers', 'the', 'round', 'pegs', 'in', 'the', 'square', 'holes', 'the', 'ones', 'who', 'see', 'things', 'differently', 'â€”', 'theyâ€™re', 'not', 'fond', 'of', 'rules', 'i', 'wanted', 'to', 'pay', 'with', 'a', 'twenty', 'dolloar', 'bill', 'however', 'but', 'she', 'couldnâ€™t', 'get', 'cash', 'i', 'like', 'brownâ€™s', 'east', 'back', 'pack', 'but', 'she', 'doesnâ€™t']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "result3 = text_to_word_sequence(obj)\n",
        "\n",
        "print('Tokenizing Words after Cleaning Punctuations: %s' %result3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KvxuBHrhler",
        "outputId": "8a415b72-fbf9-4421-d0f8-01f7aabc4314"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing Words after Cleaning Punctuations: ['hereâ€™s', 'to', 'the', 'crazy', 'ones', 'the', 'misfits', 'the', 'rebels', 'the', 'troublemakers', 'the', 'round', 'pegs', 'in', 'the', 'square', 'holes', 'the', 'ones', 'who', 'see', 'things', 'differently', 'â€”', 'theyâ€™re', 'not', 'fond', 'of', 'rules', 'i', 'wanted', 'to', 'pay', 'with', 'a', 'twenty', 'dolloar', 'bill', 'however', 'but', 'she', 'couldnâ€™t', 'get', 'cash', 'i', 'like', 'brownâ€™s', 'east', 'back', 'pack', 'but', 'she', 'doesnâ€™t']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion: \n",
        "\n",
        "í† í°í™” ì‘ì—…ì„ ë‹¨ìˆœí•˜ê²Œ ì½”í¼ìŠ¤ì—ì„œ êµ¬ë‘ì ì„ ì œì™¸í•˜ê³  ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ì˜ë¼ë‚´ëŠ” ì‘ì—…ì´ë¼ê³  ê°„ì£¼í•  ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤. \n",
        "\n",
        "1) êµ¬ë‘ì ì´ë‚˜ íŠ¹ìˆ˜ ë¬¸ìë¥¼ ë‹¨ìˆœ ì œì™¸í•´ì„œëŠ” ì•ˆ ëœë‹¤.\n",
        "ê°–ê³ ìˆëŠ” ì½”í¼ìŠ¤ì—ì„œ ë‹¨ì–´ë“¤ì„ ê±¸ëŸ¬ë‚¼ ë•Œ, êµ¬ë‘ì ì´ë‚˜ íŠ¹ìˆ˜ ë¬¸ìë¥¼ ë‹¨ìˆœíˆ ì œì™¸í•˜ëŠ” ê²ƒì€ ì˜³ì§€ ì•ŠìŠµë‹ˆë‹¤. ì½”í¼ìŠ¤ì— ëŒ€í•œ ì •ì œ ì‘ì—…ì„ ì§„í–‰í•˜ë‹¤ë³´ë©´, êµ¬ë‘ì ì¡°ì°¨ë„ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ë¶„ë¥˜í•˜ê¸°ë„ í•©ë‹ˆë‹¤. ê°€ì¥ ê¸°ë³¸ì ì¸ ì˜ˆë¥¼ ë“¤ì–´ë³´ìë©´, ë§ˆì¹¨í‘œ(.)ì™€ ê°™ì€ ê²½ìš°ëŠ” ë¬¸ì¥ì˜ ê²½ê³„ë¥¼ ì•Œ ìˆ˜ ìˆëŠ”ë° ë„ì›€ì´ ë˜ë¯€ë¡œ ë‹¨ì–´ë¥¼ ë½‘ì•„ë‚¼ ë•Œ, ë§ˆì¹¨í‘œ(.)ë¥¼ ì œì™¸í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ë˜ ë‹¤ë¥¸ ì˜ˆë¡œ ë‹¨ì–´ ìì²´ì— êµ¬ë‘ì ì„ ê°–ê³  ìˆëŠ” ê²½ìš°ë„ ìˆëŠ”ë°, m.p.hë‚˜ Ph.Dë‚˜ AT&T ê°™ì€ ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. ë˜ íŠ¹ìˆ˜ ë¬¸ìì˜ ë‹¬ëŸ¬ë‚˜ ìŠ¬ë˜ì‹œ(/)ë¡œ ì˜ˆë¥¼ ë“¤ì–´ë³´ë©´, $45.55ì™€ ê°™ì€ ê°€ê²©ì„ ì˜ë¯¸ í•˜ê¸°ë„ í•˜ê³ , 01/02/06ì€ ë‚ ì§œë¥¼ ì˜ë¯¸í•˜ê¸°ë„ í•©ë‹ˆë‹¤. ë³´í†µ ì´ëŸ° ê²½ìš° 45.55ë¥¼ í•˜ë‚˜ë¡œ ì·¨ê¸‰í•˜ê³  45ì™€ 55ë¡œ ë”°ë¡œ ë¶„ë¥˜í•˜ê³  ì‹¶ì§€ëŠ” ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ìˆ«ì ì‚¬ì´ì— ì»´ë§ˆ(,)ê°€ ë“¤ì–´ê°€ëŠ” ê²½ìš°ë„ ìˆìŠµë‹ˆë‹¤. ë³´í†µ ìˆ˜ì¹˜ë¥¼ í‘œí˜„í•  ë•ŒëŠ” 123,456,789ì™€ ê°™ì´ ì„¸ ìë¦¬ ë‹¨ìœ„ë¡œ ì»´ë§ˆê°€ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "2) ì¤„ì„ë§ê³¼ ë‹¨ì–´ ë‚´ì— ë„ì–´ì“°ê¸°ê°€ ìˆëŠ” ê²½ìš°.\n",
        "í† í°í™” ì‘ì—…ì—ì„œ ì¢…ì¢… ì˜ì–´ê¶Œ ì–¸ì–´ì˜ ì•„í¬ìŠ¤íŠ¸ë¡œí”¼(')ëŠ” ì••ì¶•ëœ ë‹¨ì–´ë¥¼ ë‹¤ì‹œ í¼ì¹˜ëŠ” ì—­í• ì„ í•˜ê¸°ë„ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ what'reëŠ” what areì˜ ì¤„ì„ë§ì´ë©°, we'reëŠ” we areì˜ ì¤„ì„ë§ì…ë‹ˆë‹¤. ìœ„ì˜ ì˜ˆì—ì„œ reë¥¼ ì ‘ì–´(clitic)ì´ë¼ê³  í•©ë‹ˆë‹¤. ì¦‰, ë‹¨ì–´ê°€ ì¤„ì„ë§ë¡œ ì“°ì¼ ë•Œ ìƒê¸°ëŠ” í˜•íƒœë¥¼ ë§í•©ë‹ˆë‹¤. ê°€ë ¹ I amì„ ì¤„ì¸ I'mì´ ìˆì„ ë•Œ, mì„ ì ‘ì–´ë¼ê³  í•©ë‹ˆë‹¤.\n",
        "\n",
        "New Yorkì´ë¼ëŠ” ë‹¨ì–´ë‚˜ rock 'n' rollì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ë´…ì‹œë‹¤. ì´ ë‹¨ì–´ë“¤ì€ í•˜ë‚˜ì˜ ë‹¨ì–´ì´ì§€ë§Œ ì¤‘ê°„ì— ë„ì–´ì“°ê¸°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ì‚¬ìš© ìš©ë„ì— ë”°ë¼ì„œ, í•˜ë‚˜ì˜ ë‹¨ì–´ ì‚¬ì´ì— ë„ì–´ì“°ê¸°ê°€ ìˆëŠ” ê²½ìš°ì—ë„ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ë´ì•¼í•˜ëŠ” ê²½ìš°ë„ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, í† í°í™” ì‘ì—…ì€ ì €ëŸ¬í•œ ë‹¨ì–´ë¥¼ í•˜ë‚˜ë¡œ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ë„ ê°€ì ¸ì•¼í•©ë‹ˆë‹¤.\n",
        "\n"
      ],
      "metadata": {
        "id": "DtdT9VC6rp-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penn Treebank Tokenizationì˜ ê·œì¹™ì— ëŒ€í•´ì„œ ì†Œê°œí•˜ê³ , í† í°í™”ì˜ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "\n",
        "  - ê·œì¹™ 1. í•˜ì´í‘¼ìœ¼ë¡œ êµ¬ì„±ëœ ë‹¨ì–´ëŠ” í•˜ë‚˜ë¡œ ìœ ì§€í•œë‹¤.\n",
        "  - ê·œì¹™ 2. doesn'tì™€ ê°™ì´ ì•„í¬ìŠ¤íŠ¸ë¡œí”¼ë¡œ 'ì ‘ì–´'ê°€ í•¨ê»˜í•˜ëŠ” ë‹¨ì–´ëŠ” ë¶„ë¦¬í•´ì¤€ë‹¤.\n",
        "\n",
        "Alert: TypeError: TreebankWordTokenizer() takes no arguments."
      ],
      "metadata": {
        "id": "x71sxg3msLG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer #Wikidocsì—ì„œëŠ” doesn't ì—ì„œ ì ‘ì–´ n't ë¥¼ ë”°ë¡œ í† í°í™” í•œë‹¤ê³  í•˜ëŠ”ë°, ì—¬ê¸° ì˜ˆì‹œì—ì„œëŠ” ê·¸ë ‡ì§€ ì•ŠìŒ. ì´ìƒí•¨...\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "result4 = tokenizer.tokenize(obj)\n",
        "\n",
        "print('íŠ¸ë¦¬ë±…í¬ ì›Œë“œí† í¬ë‚˜ì´ì € : %s' %result4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4v47eMssAMO",
        "outputId": "1080915d-0277-4b6a-f649-fe7a0b7ca38e"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "íŠ¸ë¦¬ë±…í¬ ì›Œë“œí† í¬ë‚˜ì´ì € : ['Hereâ€™s', 'to', 'the', 'crazy', 'ones', ',', 'the', 'misfits', ',', 'the', 'rebels', ',', 'the', 'troublemakers', ',', 'the', 'round', 'pegs', 'in', 'the', 'square', 'holes.', 'The', 'ones', 'who', 'see', 'things', 'differently', 'â€”', 'theyâ€™re', 'not', 'fond', 'of', 'rules.', 'I', 'wanted', 'to', 'pay', 'with', 'a', 'twenty-dolloar', 'bill', ';', 'however', ',', 'but', 'she', 'couldnâ€™t', 'get', 'cash.', 'I', 'like', 'Brownâ€™s', 'East', 'back', 'pack', ',', 'but', 'she', 'doesnâ€™t', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. ë¬¸ì¥ í† í°í™”(Sentence Tokenization)\n",
        "ì´ë²ˆì—ëŠ” í† í°ì˜ ë‹¨ìœ„ê°€ ë¬¸ì¥(sentence)ì¼ ê²½ìš°ë¥¼ ë…¼ì˜í•´ë³´ê² ìŠµë‹ˆë‹¤. ì´ ì‘ì—…ì€ ê°–ê³ ìˆëŠ” ì½”í¼ìŠ¤ ë‚´ì—ì„œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ êµ¬ë¶„í•˜ëŠ” ì‘ì—…ìœ¼ë¡œ ë•Œë¡œëŠ” ë¬¸ì¥ ë¶„ë¥˜(sentence segmentation)ë¼ê³ ë„ ë¶€ë¦…ë‹ˆë‹¤. ë³´í†µ ê°–ê³ ìˆëŠ” ì½”í¼ìŠ¤ê°€ ì •ì œë˜ì§€ ì•Šì€ ìƒíƒœë¼ë©´, ì½”í¼ìŠ¤ëŠ” ë¬¸ì¥ ë‹¨ìœ„ë¡œ êµ¬ë¶„ë˜ì–´ ìˆì§€ ì•Šì•„ì„œ ì´ë¥¼ ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” ìš©ë„ì— ë§ê²Œ ë¬¸ì¥ í† í°í™”ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì–´ë–»ê²Œ ì£¼ì–´ì§„ ì½”í¼ìŠ¤ë¡œë¶€í„° ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆì„ê¹Œìš”? ì§ê´€ì ìœ¼ë¡œ ìƒê°í•´ë´¤ì„ ë•ŒëŠ” ?ë‚˜ ë§ˆì¹¨í‘œ(.)ë‚˜ ! ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ì„ ì˜ë¼ë‚´ë©´ ë˜ì§€ ì•Šì„ê¹Œë¼ê³  ìƒê°í•  ìˆ˜ ìˆì§€ë§Œ, ê¼­ ê·¸ë ‡ì§€ë§Œì€ ì•ŠìŠµë‹ˆë‹¤. !ë‚˜ ?ëŠ” ë¬¸ì¥ì˜ êµ¬ë¶„ì„ ìœ„í•œ ê½¤ ëª…í™•í•œ êµ¬ë¶„ì(boundary) ì—­í• ì„ í•˜ì§€ë§Œ ë§ˆì¹¨í‘œëŠ” ê·¸ë ‡ì§€ ì•Šê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë§ˆì¹¨í‘œëŠ” ë¬¸ì¥ì˜ ëì´ ì•„ë‹ˆë”ë¼ë„ ë“±ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "EX1) IP 192.168.56.31 ì„œë²„ì— ë“¤ì–´ê°€ì„œ ë¡œê·¸ íŒŒì¼ ì €ì¥í•´ì„œ aaa@gmail.comë¡œ ê²°ê³¼ ì¢€ ë³´ë‚´ì¤˜. ê·¸ í›„ ì ì‹¬ ë¨¹ìœ¼ëŸ¬ ê°€ì.\n",
        "\n",
        "EX2) Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\n",
        "\n",
        "ì˜ˆë¥¼ ë“¤ì–´ ìœ„ì˜ ì˜ˆì œì— ë§ˆì¹¨í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì¥ í† í°í™”ë¥¼ ì ìš©í•´ë³¸ë‹¤ë©´ ì–´ë–¨ê¹Œìš”? ì²«ë²ˆì§¸ ì˜ˆì œì—ì„œëŠ” ë³´ë‚´ì¤˜.ì—ì„œ ê·¸ë¦¬ê³  ë‘ë²ˆì§¸ ì˜ˆì œì—ì„œëŠ” year.ì—ì„œ ì²˜ìŒìœ¼ë¡œ ë¬¸ì¥ì´ ëë‚œ ê²ƒìœ¼ë¡œ ì¸ì‹í•˜ëŠ” ê²ƒì´ ì œëŒ€ë¡œ ë¬¸ì¥ì˜ ëì„ ì˜ˆì¸¡í–ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë‹¨ìˆœíˆ ë§ˆì¹¨í‘œ(.)ë¡œ ë¬¸ì¥ì„ êµ¬ë¶„ì§“ëŠ”ë‹¤ê³  ê°€ì •í•˜ë©´, ë¬¸ì¥ì˜ ëì´ ë‚˜ì˜¤ê¸° ì „ì— ì´ë¯¸ ë§ˆì¹¨í‘œê°€ ì—¬ëŸ¬ë²ˆ ë“±ì¥í•˜ì—¬ ì˜ˆìƒí•œ ê²°ê³¼ê°€ ë‚˜ì˜¤ì§€ ì•Šê²Œ ë©ë‹ˆë‹¤.\n",
        "\n",
        "ì‚¬ìš©í•˜ëŠ” ì½”í¼ìŠ¤ê°€ ì–´ë–¤ êµ­ì ì˜ ì–¸ì–´ì¸ì§€, ë˜ëŠ” í•´ë‹¹ ì½”í¼ìŠ¤ ë‚´ì—ì„œ íŠ¹ìˆ˜ë¬¸ìë“¤ì´ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ê³  ìˆëŠ”ì§€ì— ë”°ë¼ì„œ ì§ì ‘ ê·œì¹™ë“¤ì„ ì •ì˜í•´ë³¼ ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤. 100% ì •í™•ë„ë¥¼ ì–»ëŠ” ì¼ì€ ì‰¬ìš´ ì¼ì´ ì•„ë‹Œë°, ê°–ê³ ìˆëŠ” ì½”í¼ìŠ¤ ë°ì´í„°ì— ì˜¤íƒ€ë‚˜, ë¬¸ì¥ì˜ êµ¬ì„±ì´ ì—‰ë§ì´ë¼ë©´ ì •í•´ë†“ì€ ê·œì¹™ì´ ì†Œìš©ì´ ì—†ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
        "\n",
        "NLTKì—ì„œëŠ” ì˜ì–´ ë¬¸ì¥ì˜ í† í°í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” sent_tokenizeë¥¼ ì§€ì›í•˜ê³  ìˆìŠµë‹ˆë‹¤. NLTKë¥¼ í†µí•´ ë¬¸ì¥ í† í°í™”ë¥¼ ì‹¤ìŠµí•´ë³´ê² ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "PHOoPaU2OkGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ¹ sent_tokenize()\n",
        "\n",
        "* I am not happy with results with conjunctions, post-numbers, etc. "
      ],
      "metadata": {
        "id": "eZ37_yhXOmFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk \n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sentences = \"I'm actively looking for Ph.D. students. \\\n",
        "and you are a Ph.D student. \\\n",
        "Visit IP 192.168.56.31 \\\n",
        "and send the results to my email account. \\\n",
        "It's email@gmail.com.\"\n",
        "\n",
        "sentence = sent_tokenize(sentences)\n",
        "print('ë¬¸ì¥ í† í°í™”: %s' %sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv8s_PN-OlG_",
        "outputId": "c2bd4eaf-1aa9-4786-ddac-19d08768c628"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ë¬¸ì¥ í† í°í™”: [\"I'm actively looking for Ph.D. students.\", 'and you are a Ph.D student.', 'Visit IP 192.168.56.31 and send the results to my email account.', \"It's email@gmail.com.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence tokenizer for Korean: kss package"
      ],
      "metadata": {
        "id": "xGOApXRERwUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kss\n",
        "import kss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHP16yVbR4Z4",
        "outputId": "fd980c6a-5caa-42a8-b3e6-b4fce8ba0db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kss\n",
            "  Using cached kss-4.5.3.tar.gz (78 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting emoji==1.2.0 (from kss)\n",
            "  Using cached emoji-1.2.0-py3-none-any.whl (131 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from kss) (2022.10.31)\n",
            "Collecting pecab (from kss)\n",
            "  Using cached pecab-1.0.8.tar.gz (26.4 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from kss) (3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (1.22.4)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (9.0.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (7.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.1.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.1)\n",
            "Building wheels for collected packages: kss, pecab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "korfl = ('/content/sample_data/Exercise/Squall.txt', 'rt')\n",
        "korfl.read()"
      ],
      "metadata": {
        "id": "RQez4XkpTjSE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}