{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGyWg56eXCbUzmA4N4e8pH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms624atyale/Python_Basics/blob/main/28_Tokenization_VariousWays.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üçî üçü üç©  <font color = 'blue'> **Under construction**\n",
        "\n",
        "### <font color = 'red'> **Corpus data (e.g., crawling) should be preprocessed before further analysis by means of Cleaning(Ï†ïÏ†ú), Tokenization(ÌÜ†ÌÅ∞Ìôî), Normalization(Ï†ïÍ∑úÌôî)**. \n",
        "* [English Tokenization](https://wikidocs.net/21698)\n",
        "\n",
        "## Cleaning\n",
        "*Íµ¨ÎëêÏ†ê(punctuation (e.g., \".\", \",\", \"?\", \"!\", \";\", \":\")ÏùÑ ÏßÄÏö∞Í∏∞\n",
        "\n",
        "*ÌäπÏàòÎ¨∏Ïûê Î™®Îëê ÏßÄÏö∞Í∏∞ \n",
        "\n",
        "\n",
        "## Tokenization \n",
        "\n",
        "* Tokenization: Ï£ºÏñ¥ÏßÑ ÏΩîÌçºÏä§(corpus)ÏóêÏÑú ÌÜ†ÌÅ∞(token)Ïù¥Îùº Î∂àÎ¶¨Îäî Îã®ÏúÑ (e.g., word, phrase, strings with meaning)Î°ú ÎÇòÎàÑÎäî ÏûëÏóÖ\n",
        "* ÌÜ†ÌÅ∞Ïùò Îã®ÏúÑÍ∞Ä ÏÉÅÌô©Ïóê Îî∞Îùº Îã§Î•¥ÏßÄÎßå, Î≥¥ÌÜµ ÏùòÎØ∏ÏûàÎäî Îã®ÏúÑÎ°ú ÌÜ†ÌÅ∞ÏùÑ Ï†ïÏùòÌï©ÎãàÎã§. \n",
        "* NLTK, KoNLPYÎ•º ÌÜµÌï¥ Ïã§ÏäµÏùÑ ÏßÑÌñâÌïòÎ©∞ ÌÜ†ÌÅ∞Ìôî\n",
        "\n",
        "Simplest tokenization: Íµ¨ÎëêÏ†ê ÏßÄÏö¥ ÌõÑ ÎùÑÏñ¥Ïì∞Í∏∞(whitespace)Î•º Í∏∞Ï§ÄÏúºÎ°ú ÏûòÎùºÎÇ¥Í∏∞ Ï†úÏô∏\n"
      ],
      "metadata": {
        "id": "gb1sh28mTSox"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "qWcgp3AzSyi4"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Here‚Äôs to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. \\\n",
        "The ones who see things differently ‚Äî they‚Äôre not fond of rules. \\\n",
        "I wanted to pay with a twenty-dolloar bill; however, she couldn‚Äôt get cash. \\\n",
        "I like Brown‚Äôs East back pack,\\\n",
        "but she doesn‚Äôt.'"
      ],
      "metadata": {
        "id": "4odDoSnPS18k"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QovcT0MjS7gp",
        "outputId": "f4327565-7023-4a02-a8fe-3746ad357d5f"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here‚Äôs to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. The ones who see things differently ‚Äî they‚Äôre not fond of rules. I wanted to pay with a twenty-dolloar bill; however, she couldn‚Äôt get cash. I like Brown‚Äôs East back pack,but she doesn‚Äôt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/sample_data/Exercise/text_symbol_sample.txt','rt')\n",
        "file.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "gbFvGofSTFDR",
        "outputId": "be3479a1-543e-411c-c911-b0c117086c92"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here‚Äôs to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes.\\nThe ones who see things differently ‚Äî they‚Äôre not fond of rules. \\nI wanted to pay with a twenty-dolloar bill; \\nhowever, but she couldn‚Äôt get cash. \\nI like Brown‚Äôs East back pack,\\nbut she doesn‚Äôt.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/sample_data/Exercise/text_symbol_sample.txt','rt')\n",
        "file.read().replace(\"\\n\", \" \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "afJEPVK-0zZl",
        "outputId": "8e35cf06-c645-4df6-e670-ed479ebde1c3"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here‚Äôs to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. The ones who see things differently ‚Äî they‚Äôre not fond of rules.  I wanted to pay with a twenty-dolloar bill;  however, but she couldn‚Äôt get cash.  I like Brown‚Äôs East back pack, but she doesn‚Äôt.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simplest way to tokenize a text into units."
      ],
      "metadata": {
        "id": "sTnGhW7YrT_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/sample_data/Exercise/text_symbol_sample.txt','rt')\n",
        "obj = file.read().replace(\"\\n\", \" \")\n",
        "obj.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWBNRkayhJeF",
        "outputId": "179d1e5b-8d39-4b81-ddec-1a4134597d3f"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Here‚Äôs',\n",
              " 'to',\n",
              " 'the',\n",
              " 'crazy',\n",
              " 'ones,',\n",
              " 'the',\n",
              " 'misfits,',\n",
              " 'the',\n",
              " 'rebels,',\n",
              " 'the',\n",
              " 'troublemakers,',\n",
              " 'the',\n",
              " 'round',\n",
              " 'pegs',\n",
              " 'in',\n",
              " 'the',\n",
              " 'square',\n",
              " 'holes.',\n",
              " 'The',\n",
              " 'ones',\n",
              " 'who',\n",
              " 'see',\n",
              " 'things',\n",
              " 'differently',\n",
              " '‚Äî',\n",
              " 'they‚Äôre',\n",
              " 'not',\n",
              " 'fond',\n",
              " 'of',\n",
              " 'rules.',\n",
              " 'I',\n",
              " 'wanted',\n",
              " 'to',\n",
              " 'pay',\n",
              " 'with',\n",
              " 'a',\n",
              " 'twenty-dolloar',\n",
              " 'bill;',\n",
              " 'however,',\n",
              " 'but',\n",
              " 'she',\n",
              " 'couldn‚Äôt',\n",
              " 'get',\n",
              " 'cash.',\n",
              " 'I',\n",
              " 'like',\n",
              " 'Brown‚Äôs',\n",
              " 'East',\n",
              " 'back',\n",
              " 'pack,',\n",
              " 'but',\n",
              " 'she',\n",
              " 'doesn‚Äôt.']"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üêπ nltk.tokenize.**word_tokenize()**\n",
        "\n",
        "* <font color = 'sky blue'> Punctuations as well as symbols such as a phrase-linking hiphen\"-\" and an apostrophe \"'\"(perfect tense, possessive) are tokenized.\n",
        "* <font color = 'sky blue'> cf., A within-word hiphen belongs to a word (e.g., 'ten-dollar'), not being tokenized on its own ."
      ],
      "metadata": {
        "id": "A03RIU6gnLZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "\n",
        "print('Tokenizing Word and Punctuation:',word_tokenize(obj)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9Pfw0_Thhka",
        "outputId": "b67a2967-611f-4d83-b25f-91728c2a055e"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing Word and Punctuation: ['Here', '‚Äô', 's', 'to', 'the', 'crazy', 'ones', ',', 'the', 'misfits', ',', 'the', 'rebels', ',', 'the', 'troublemakers', ',', 'the', 'round', 'pegs', 'in', 'the', 'square', 'holes', '.', 'The', 'ones', 'who', 'see', 'things', 'differently', '‚Äî', 'they', '‚Äô', 're', 'not', 'fond', 'of', 'rules', '.', 'I', 'wanted', 'to', 'pay', 'with', 'a', 'twenty-dolloar', 'bill', ';', 'however', ',', 'but', 'she', 'couldn', '‚Äô', 't', 'get', 'cash', '.', 'I', 'like', 'Brown', '‚Äô', 's', 'East', 'back', 'pack', ',', 'but', 'she', 'doesn', '‚Äô', 't', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Advanced for a print codeline.\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "result1 = word_tokenize(obj)\n",
        "\n",
        "print('Tokenizing Word and Punctuation: %s' %result1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1NIz986mFTA",
        "outputId": "c99f1a3a-cc16-4c5a-ec74-23875b82e218"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing Word and Punctuation: ['Here', '‚Äô', 's', 'to', 'the', 'crazy', 'ones', ',', 'the', 'misfits', ',', 'the', 'rebels', ',', 'the', 'troublemakers', ',', 'the', 'round', 'pegs', 'in', 'the', 'square', 'holes', '.', 'The', 'ones', 'who', 'see', 'things', 'differently', '‚Äî', 'they', '‚Äô', 're', 'not', 'fond', 'of', 'rules', '.', 'I', 'wanted', 'to', 'pay', 'with', 'a', 'twenty-dolloar', 'bill', ';', 'however', ',', 'but', 'she', 'couldn', '‚Äô', 't', 'get', 'cash', '.', 'I', 'like', 'Brown', '‚Äô', 's', 'East', 'back', 'pack', ',', 'but', 'she', 'doesn', '‚Äô', 't', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üêπ nltk.tokenize.**WordPunctTokenizer()**\n",
        "\n",
        "* <font color = 'sky blue'> Punctuations (\".\", \",\", \".\", \";\"), a phrase-linking hypthen (\"-\"), a word-linking hyphen (\"-\"), and an apostrophe (\"'\") (perfect tense, possessive, negation)) are tokenized as a unit across the board.\n",
        "\n",
        "  * <font color = 'pink'> Wrong command: >>WordPunctTokenizer(txt) or WordPunctTokenizer.tokenize(txt): Error message: WordPunctTokenizer.__init__() takes 1 positional argument but 2 were given."
      ],
      "metadata": {
        "id": "lOu9-q_Cn01a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "print('Tokenizing Words and Punctuations:', WordPunctTokenizer().tokenize(obj)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYAWzvgbhiwd",
        "outputId": "2740aeb6-b8db-494a-c0be-4e5cb747ca8d"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing Words and Punctuations: ['Here', '‚Äô', 's', 'to', 'the', 'crazy', 'ones', ',', 'the', 'misfits', ',', 'the', 'rebels', ',', 'the', 'troublemakers', ',', 'the', 'round', 'pegs', 'in', 'the', 'square', 'holes', '.', 'The', 'ones', 'who', 'see', 'things', 'differently', '‚Äî', 'they', '‚Äô', 're', 'not', 'fond', 'of', 'rules', '.', 'I', 'wanted', 'to', 'pay', 'with', 'a', 'twenty', '-', 'dolloar', 'bill', ';', 'however', ',', 'but', 'she', 'couldn', '‚Äô', 't', 'get', 'cash', '.', 'I', 'like', 'Brown', '‚Äô', 's', 'East', 'back', 'pack', ',', 'but', 'she', 'doesn', '‚Äô', 't', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "result2 = WordPunctTokenizer().tokenize(obj)\n",
        "\n",
        "print('Tokenizing Words and Punctuations: %s' %result2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kfxxv5cImhFa",
        "outputId": "eab5b638-5015-41f8-c980-3385808cc55f"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing Words and Punctuations: ['Here', '‚Äô', 's', 'to', 'the', 'crazy', 'ones', ',', 'the', 'misfits', ',', 'the', 'rebels', ',', 'the', 'troublemakers', ',', 'the', 'round', 'pegs', 'in', 'the', 'square', 'holes', '.', 'The', 'ones', 'who', 'see', 'things', 'differently', '‚Äî', 'they', '‚Äô', 're', 'not', 'fond', 'of', 'rules', '.', 'I', 'wanted', 'to', 'pay', 'with', 'a', 'twenty', '-', 'dolloar', 'bill', ';', 'however', ',', 'but', 'she', 'couldn', '‚Äô', 't', 'get', 'cash', '.', 'I', 'like', 'Brown', '‚Äô', 's', 'East', 'back', 'pack', ',', 'but', 'she', 'doesn', '‚Äô', 't', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üêπ tensorflow.keras.preprocessing.text.**text_to_word_sequence()**\n",
        "\n",
        "* <font color = 'sky blue'> Small letters across the board\n",
        "* An apostrophe (\"'\" for Perfect tense (e.g., \"I've\"), possessive (e.g., \"John's\"), negation (e.g., 'doesn't)) is part of a token.\n",
        "* A phrase-linking hyphen (-) are tokenized. \n",
        "* **All punctuations and a word-linking phyphen are deleted**. \n"
      ],
      "metadata": {
        "id": "boMBjWuFptSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "print('Tokenizing Words after Cleaning Punctuations:', text_to_word_sequence(obj))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aowUE2sTrAi6",
        "outputId": "a8171414-5888-478d-d95e-9131ea072c3c"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing Words after Cleaning Punctuations: ['here‚Äôs', 'to', 'the', 'crazy', 'ones', 'the', 'misfits', 'the', 'rebels', 'the', 'troublemakers', 'the', 'round', 'pegs', 'in', 'the', 'square', 'holes', 'the', 'ones', 'who', 'see', 'things', 'differently', '‚Äî', 'they‚Äôre', 'not', 'fond', 'of', 'rules', 'i', 'wanted', 'to', 'pay', 'with', 'a', 'twenty', 'dolloar', 'bill', 'however', 'but', 'she', 'couldn‚Äôt', 'get', 'cash', 'i', 'like', 'brown‚Äôs', 'east', 'back', 'pack', 'but', 'she', 'doesn‚Äôt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "result3 = text_to_word_sequence(obj)\n",
        "\n",
        "print('Tokenizing Words after Cleaning Punctuations: %s' %result3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KvxuBHrhler",
        "outputId": "8a415b72-fbf9-4421-d0f8-01f7aabc4314"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing Words after Cleaning Punctuations: ['here‚Äôs', 'to', 'the', 'crazy', 'ones', 'the', 'misfits', 'the', 'rebels', 'the', 'troublemakers', 'the', 'round', 'pegs', 'in', 'the', 'square', 'holes', 'the', 'ones', 'who', 'see', 'things', 'differently', '‚Äî', 'they‚Äôre', 'not', 'fond', 'of', 'rules', 'i', 'wanted', 'to', 'pay', 'with', 'a', 'twenty', 'dolloar', 'bill', 'however', 'but', 'she', 'couldn‚Äôt', 'get', 'cash', 'i', 'like', 'brown‚Äôs', 'east', 'back', 'pack', 'but', 'she', 'doesn‚Äôt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion: \n",
        "\n",
        "ÌÜ†ÌÅ∞Ìôî ÏûëÏóÖÏùÑ Îã®ÏàúÌïòÍ≤å ÏΩîÌçºÏä§ÏóêÏÑú Íµ¨ÎëêÏ†êÏùÑ Ï†úÏô∏ÌïòÍ≥† Í≥µÎ∞± Í∏∞Ï§ÄÏúºÎ°ú ÏûòÎùºÎÇ¥Îäî ÏûëÏóÖÏù¥ÎùºÍ≥† Í∞ÑÏ£ºÌï† ÏàòÎäî ÏóÜÏäµÎãàÎã§. \n",
        "\n",
        "1) Íµ¨ÎëêÏ†êÏù¥ÎÇò ÌäπÏàò Î¨∏ÏûêÎ•º Îã®Ïàú Ï†úÏô∏Ìï¥ÏÑúÎäî Ïïà ÎêúÎã§.\n",
        "Í∞ñÍ≥†ÏûàÎäî ÏΩîÌçºÏä§ÏóêÏÑú Îã®Ïñ¥Îì§ÏùÑ Í±∏Îü¨ÎÇº Îïå, Íµ¨ÎëêÏ†êÏù¥ÎÇò ÌäπÏàò Î¨∏ÏûêÎ•º Îã®ÏàúÌûà Ï†úÏô∏ÌïòÎäî Í≤ÉÏùÄ Ïò≥ÏßÄ ÏïäÏäµÎãàÎã§. ÏΩîÌçºÏä§Ïóê ÎåÄÌïú Ï†ïÏ†ú ÏûëÏóÖÏùÑ ÏßÑÌñâÌïòÎã§Î≥¥Î©¥, Íµ¨ÎëêÏ†êÏ°∞Ï∞®ÎèÑ ÌïòÎÇòÏùò ÌÜ†ÌÅ∞ÏúºÎ°ú Î∂ÑÎ•òÌïòÍ∏∞ÎèÑ Ìï©ÎãàÎã§. Í∞ÄÏû• Í∏∞Î≥∏Ï†ÅÏù∏ ÏòàÎ•º Îì§Ïñ¥Î≥¥ÏûêÎ©¥, ÎßàÏπ®Ìëú(.)ÏôÄ Í∞ôÏùÄ Í≤ΩÏö∞Îäî Î¨∏Ïû•Ïùò Í≤ΩÍ≥ÑÎ•º Ïïå Ïàò ÏûàÎäîÎç∞ ÎèÑÏõÄÏù¥ ÎêòÎØÄÎ°ú Îã®Ïñ¥Î•º ÎΩëÏïÑÎÇº Îïå, ÎßàÏπ®Ìëú(.)Î•º Ï†úÏô∏ÌïòÏßÄ ÏïäÏùÑ Ïàò ÏûàÏäµÎãàÎã§.\n",
        "\n",
        "Îòê Îã§Î•∏ ÏòàÎ°ú Îã®Ïñ¥ ÏûêÏ≤¥Ïóê Íµ¨ÎëêÏ†êÏùÑ Í∞ñÍ≥† ÏûàÎäî Í≤ΩÏö∞ÎèÑ ÏûàÎäîÎç∞, m.p.hÎÇò Ph.DÎÇò AT&T Í∞ôÏùÄ Í≤ΩÏö∞Í∞Ä ÏûàÏäµÎãàÎã§. Îòê ÌäπÏàò Î¨∏ÏûêÏùò Îã¨Îü¨ÎÇò Ïä¨ÎûòÏãú(/)Î°ú ÏòàÎ•º Îì§Ïñ¥Î≥¥Î©¥, $45.55ÏôÄ Í∞ôÏùÄ Í∞ÄÍ≤©ÏùÑ ÏùòÎØ∏ ÌïòÍ∏∞ÎèÑ ÌïòÍ≥†, 01/02/06ÏùÄ ÎÇ†ÏßúÎ•º ÏùòÎØ∏ÌïòÍ∏∞ÎèÑ Ìï©ÎãàÎã§. Î≥¥ÌÜµ Ïù¥Îü∞ Í≤ΩÏö∞ 45.55Î•º ÌïòÎÇòÎ°ú Ï∑®Í∏âÌïòÍ≥† 45ÏôÄ 55Î°ú Îî∞Î°ú Î∂ÑÎ•òÌïòÍ≥† Ïã∂ÏßÄÎäî ÏïäÏùÑ Ïàò ÏûàÏäµÎãàÎã§.\n",
        "\n",
        "Ïà´Ïûê ÏÇ¨Ïù¥Ïóê Ïª¥Îßà(,)Í∞Ä Îì§Ïñ¥Í∞ÄÎäî Í≤ΩÏö∞ÎèÑ ÏûàÏäµÎãàÎã§. Î≥¥ÌÜµ ÏàòÏπòÎ•º ÌëúÌòÑÌï† ÎïåÎäî 123,456,789ÏôÄ Í∞ôÏù¥ ÏÑ∏ ÏûêÎ¶¨ Îã®ÏúÑÎ°ú Ïª¥ÎßàÍ∞Ä ÏûàÏäµÎãàÎã§.\n",
        "\n",
        "2) Ï§ÑÏûÑÎßêÍ≥º Îã®Ïñ¥ ÎÇ¥Ïóê ÎùÑÏñ¥Ïì∞Í∏∞Í∞Ä ÏûàÎäî Í≤ΩÏö∞.\n",
        "ÌÜ†ÌÅ∞Ìôî ÏûëÏóÖÏóêÏÑú Ï¢ÖÏ¢Ö ÏòÅÏñ¥Í∂å Ïñ∏Ïñ¥Ïùò ÏïÑÌè¨Ïä§Ìä∏Î°úÌîº(')Îäî ÏïïÏ∂ïÎêú Îã®Ïñ¥Î•º Îã§Ïãú ÌéºÏπòÎäî Ïó≠Ìï†ÏùÑ ÌïòÍ∏∞ÎèÑ Ìï©ÎãàÎã§. ÏòàÎ•º Îì§Ïñ¥ what'reÎäî what areÏùò Ï§ÑÏûÑÎßêÏù¥Î©∞, we'reÎäî we areÏùò Ï§ÑÏûÑÎßêÏûÖÎãàÎã§. ÏúÑÏùò ÏòàÏóêÏÑú reÎ•º Ï†ëÏñ¥(clitic)Ïù¥ÎùºÍ≥† Ìï©ÎãàÎã§. Ï¶â, Îã®Ïñ¥Í∞Ä Ï§ÑÏûÑÎßêÎ°ú Ïì∞Ïùº Îïå ÏÉùÍ∏∞Îäî ÌòïÌÉúÎ•º ÎßêÌï©ÎãàÎã§. Í∞ÄÎ†π I amÏùÑ Ï§ÑÏù∏ I'mÏù¥ ÏûàÏùÑ Îïå, mÏùÑ Ï†ëÏñ¥ÎùºÍ≥† Ìï©ÎãàÎã§.\n",
        "\n",
        "New YorkÏù¥ÎùºÎäî Îã®Ïñ¥ÎÇò rock 'n' rollÏù¥ÎùºÎäî Îã®Ïñ¥Î•º Î¥ÖÏãúÎã§. Ïù¥ Îã®Ïñ¥Îì§ÏùÄ ÌïòÎÇòÏùò Îã®Ïñ¥Ïù¥ÏßÄÎßå Ï§ëÍ∞ÑÏóê ÎùÑÏñ¥Ïì∞Í∏∞Í∞Ä Ï°¥Ïû¨Ìï©ÎãàÎã§. ÏÇ¨Ïö© Ïö©ÎèÑÏóê Îî∞ÎùºÏÑú, ÌïòÎÇòÏùò Îã®Ïñ¥ ÏÇ¨Ïù¥Ïóê ÎùÑÏñ¥Ïì∞Í∏∞Í∞Ä ÏûàÎäî Í≤ΩÏö∞ÏóêÎèÑ ÌïòÎÇòÏùò ÌÜ†ÌÅ∞ÏúºÎ°ú Î¥êÏïºÌïòÎäî Í≤ΩÏö∞ÎèÑ ÏûàÏùÑ Ïàò ÏûàÏúºÎØÄÎ°ú, ÌÜ†ÌÅ∞Ìôî ÏûëÏóÖÏùÄ Ï†ÄÎü¨Ìïú Îã®Ïñ¥Î•º ÌïòÎÇòÎ°ú Ïù∏ÏãùÌï† Ïàò ÏûàÎäî Îä•Î†•ÎèÑ Í∞ÄÏ†∏ÏïºÌï©ÎãàÎã§.\n",
        "\n"
      ],
      "metadata": {
        "id": "DtdT9VC6rp-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Penn Treebank TokenizationÏùò Í∑úÏπôÏóê ÎåÄÌï¥ÏÑú ÏÜåÍ∞úÌïòÍ≥†, ÌÜ†ÌÅ∞ÌôîÏùò Í≤∞Í≥ºÎ•º ÌôïÏù∏Ìï¥Î≥¥Í≤†ÏäµÎãàÎã§.\n",
        "\n",
        "  - Í∑úÏπô 1. ÌïòÏù¥ÌëºÏúºÎ°ú Íµ¨ÏÑ±Îêú Îã®Ïñ¥Îäî ÌïòÎÇòÎ°ú Ïú†ÏßÄÌïúÎã§.\n",
        "  - Í∑úÏπô 2. doesn'tÏôÄ Í∞ôÏù¥ ÏïÑÌè¨Ïä§Ìä∏Î°úÌîºÎ°ú 'Ï†ëÏñ¥'Í∞Ä Ìï®ÍªòÌïòÎäî Îã®Ïñ¥Îäî Î∂ÑÎ¶¨Ìï¥Ï§ÄÎã§.\n",
        "\n",
        "Alert: TypeError: TreebankWordTokenizer() takes no arguments."
      ],
      "metadata": {
        "id": "x71sxg3msLG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer #WikidocsÏóêÏÑúÎäî doesn't ÏóêÏÑú Ï†ëÏñ¥ n't Î•º Îî∞Î°ú ÌÜ†ÌÅ∞Ìôî ÌïúÎã§Í≥† ÌïòÎäîÎç∞, Ïó¨Í∏∞ ÏòàÏãúÏóêÏÑúÎäî Í∑∏Î†áÏßÄ ÏïäÏùå. Ïù¥ÏÉÅÌï®...\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "result4 = tokenizer.tokenize(obj)\n",
        "\n",
        "print('Ìä∏Î¶¨Î±ÖÌÅ¨ ÏõåÎìúÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä : %s' %result4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4v47eMssAMO",
        "outputId": "1080915d-0277-4b6a-f649-fe7a0b7ca38e"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ìä∏Î¶¨Î±ÖÌÅ¨ ÏõåÎìúÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä : ['Here‚Äôs', 'to', 'the', 'crazy', 'ones', ',', 'the', 'misfits', ',', 'the', 'rebels', ',', 'the', 'troublemakers', ',', 'the', 'round', 'pegs', 'in', 'the', 'square', 'holes.', 'The', 'ones', 'who', 'see', 'things', 'differently', '‚Äî', 'they‚Äôre', 'not', 'fond', 'of', 'rules.', 'I', 'wanted', 'to', 'pay', 'with', 'a', 'twenty-dolloar', 'bill', ';', 'however', ',', 'but', 'she', 'couldn‚Äôt', 'get', 'cash.', 'I', 'like', 'Brown‚Äôs', 'East', 'back', 'pack', ',', 'but', 'she', 'doesn‚Äôt', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Î¨∏Ïû• ÌÜ†ÌÅ∞Ìôî(Sentence Tokenization)\n",
        "Ïù¥Î≤àÏóêÎäî ÌÜ†ÌÅ∞Ïùò Îã®ÏúÑÍ∞Ä Î¨∏Ïû•(sentence)Ïùº Í≤ΩÏö∞Î•º ÎÖºÏùòÌï¥Î≥¥Í≤†ÏäµÎãàÎã§. Ïù¥ ÏûëÏóÖÏùÄ Í∞ñÍ≥†ÏûàÎäî ÏΩîÌçºÏä§ ÎÇ¥ÏóêÏÑú Î¨∏Ïû• Îã®ÏúÑÎ°ú Íµ¨Î∂ÑÌïòÎäî ÏûëÏóÖÏúºÎ°ú ÎïåÎ°úÎäî Î¨∏Ïû• Î∂ÑÎ•ò(sentence segmentation)ÎùºÍ≥†ÎèÑ Î∂ÄÎ¶ÖÎãàÎã§. Î≥¥ÌÜµ Í∞ñÍ≥†ÏûàÎäî ÏΩîÌçºÏä§Í∞Ä Ï†ïÏ†úÎêòÏßÄ ÏïäÏùÄ ÏÉÅÌÉúÎùºÎ©¥, ÏΩîÌçºÏä§Îäî Î¨∏Ïû• Îã®ÏúÑÎ°ú Íµ¨Î∂ÑÎêòÏñ¥ ÏûàÏßÄ ÏïäÏïÑÏÑú Ïù¥Î•º ÏÇ¨Ïö©ÌïòÍ≥†Ïûê ÌïòÎäî Ïö©ÎèÑÏóê ÎßûÍ≤å Î¨∏Ïû• ÌÜ†ÌÅ∞ÌôîÍ∞Ä ÌïÑÏöîÌï† Ïàò ÏûàÏäµÎãàÎã§.\n",
        "\n",
        "Ïñ¥ÎñªÍ≤å Ï£ºÏñ¥ÏßÑ ÏΩîÌçºÏä§Î°úÎ∂ÄÌÑ∞ Î¨∏Ïû• Îã®ÏúÑÎ°ú Î∂ÑÎ•òÌï† Ïàò ÏûàÏùÑÍπåÏöî? ÏßÅÍ¥ÄÏ†ÅÏúºÎ°ú ÏÉùÍ∞ÅÌï¥Î¥§ÏùÑ ÎïåÎäî ?ÎÇò ÎßàÏπ®Ìëú(.)ÎÇò ! Í∏∞Ï§ÄÏúºÎ°ú Î¨∏Ïû•ÏùÑ ÏûòÎùºÎÇ¥Î©¥ ÎêòÏßÄ ÏïäÏùÑÍπåÎùºÍ≥† ÏÉùÍ∞ÅÌï† Ïàò ÏûàÏßÄÎßå, Íº≠ Í∑∏Î†áÏßÄÎßåÏùÄ ÏïäÏäµÎãàÎã§. !ÎÇò ?Îäî Î¨∏Ïû•Ïùò Íµ¨Î∂ÑÏùÑ ÏúÑÌïú ÍΩ§ Î™ÖÌôïÌïú Íµ¨Î∂ÑÏûê(boundary) Ïó≠Ìï†ÏùÑ ÌïòÏßÄÎßå ÎßàÏπ®ÌëúÎäî Í∑∏Î†áÏßÄ ÏïäÍ∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§. ÎßàÏπ®ÌëúÎäî Î¨∏Ïû•Ïùò ÎÅùÏù¥ ÏïÑÎãàÎçîÎùºÎèÑ Îì±Ïû•Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n",
        "\n",
        "EX1) IP 192.168.56.31 ÏÑúÎ≤ÑÏóê Îì§Ïñ¥Í∞ÄÏÑú Î°úÍ∑∏ ÌååÏùº Ï†ÄÏû•Ìï¥ÏÑú aaa@gmail.comÎ°ú Í≤∞Í≥º Ï¢Ä Î≥¥ÎÇ¥Ï§ò. Í∑∏ ÌõÑ Ï†êÏã¨ Î®πÏúºÎü¨ Í∞ÄÏûê.\n",
        "\n",
        "EX2) Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\n",
        "\n",
        "ÏòàÎ•º Îì§Ïñ¥ ÏúÑÏùò ÏòàÏ†úÏóê ÎßàÏπ®ÌëúÎ•º Í∏∞Ï§ÄÏúºÎ°ú Î¨∏Ïû• ÌÜ†ÌÅ∞ÌôîÎ•º Ï†ÅÏö©Ìï¥Î≥∏Îã§Î©¥ Ïñ¥Îñ®ÍπåÏöî? Ï≤´Î≤àÏß∏ ÏòàÏ†úÏóêÏÑúÎäî Î≥¥ÎÇ¥Ï§ò.ÏóêÏÑú Í∑∏Î¶¨Í≥† ÎëêÎ≤àÏß∏ ÏòàÏ†úÏóêÏÑúÎäî year.ÏóêÏÑú Ï≤òÏùåÏúºÎ°ú Î¨∏Ïû•Ïù¥ ÎÅùÎÇú Í≤ÉÏúºÎ°ú Ïù∏ÏãùÌïòÎäî Í≤ÉÏù¥ Ï†úÎåÄÎ°ú Î¨∏Ïû•Ïùò ÎÅùÏùÑ ÏòàÏ∏°ÌñàÎã§Í≥† Î≥º Ïàò ÏûàÏäµÎãàÎã§. ÌïòÏßÄÎßå Îã®ÏàúÌûà ÎßàÏπ®Ìëú(.)Î°ú Î¨∏Ïû•ÏùÑ Íµ¨Î∂ÑÏßìÎäîÎã§Í≥† Í∞ÄÏ†ïÌïòÎ©¥, Î¨∏Ïû•Ïùò ÎÅùÏù¥ ÎÇòÏò§Í∏∞ Ï†ÑÏóê Ïù¥ÎØ∏ ÎßàÏπ®ÌëúÍ∞Ä Ïó¨Îü¨Î≤à Îì±Ïû•ÌïòÏó¨ ÏòàÏÉÅÌïú Í≤∞Í≥ºÍ∞Ä ÎÇòÏò§ÏßÄ ÏïäÍ≤å Îê©ÎãàÎã§.\n",
        "\n",
        "ÏÇ¨Ïö©ÌïòÎäî ÏΩîÌçºÏä§Í∞Ä Ïñ¥Îñ§ Íµ≠Ï†ÅÏùò Ïñ∏Ïñ¥Ïù∏ÏßÄ, ÎòêÎäî Ìï¥Îãπ ÏΩîÌçºÏä§ ÎÇ¥ÏóêÏÑú ÌäπÏàòÎ¨∏ÏûêÎì§Ïù¥ Ïñ¥ÎñªÍ≤å ÏÇ¨Ïö©ÎêòÍ≥† ÏûàÎäîÏßÄÏóê Îî∞ÎùºÏÑú ÏßÅÏ†ë Í∑úÏπôÎì§ÏùÑ Ï†ïÏùòÌï¥Î≥º Ïàò ÏûàÍ≤†ÏäµÎãàÎã§. 100% Ï†ïÌôïÎèÑÎ•º ÏñªÎäî ÏùºÏùÄ Ïâ¨Ïö¥ ÏùºÏù¥ ÏïÑÎãåÎç∞, Í∞ñÍ≥†ÏûàÎäî ÏΩîÌçºÏä§ Îç∞Ïù¥ÌÑ∞Ïóê Ïò§ÌÉÄÎÇò, Î¨∏Ïû•Ïùò Íµ¨ÏÑ±Ïù¥ ÏóâÎßùÏù¥ÎùºÎ©¥ Ï†ïÌï¥ÎÜìÏùÄ Í∑úÏπôÏù¥ ÏÜåÏö©Ïù¥ ÏóÜÏùÑ Ïàò ÏûàÍ∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§.\n",
        "\n",
        "NLTKÏóêÏÑúÎäî ÏòÅÏñ¥ Î¨∏Ïû•Ïùò ÌÜ†ÌÅ∞ÌôîÎ•º ÏàòÌñâÌïòÎäî sent_tokenizeÎ•º ÏßÄÏõêÌïòÍ≥† ÏûàÏäµÎãàÎã§. NLTKÎ•º ÌÜµÌï¥ Î¨∏Ïû• ÌÜ†ÌÅ∞ÌôîÎ•º Ïã§ÏäµÌï¥Î≥¥Í≤†ÏäµÎãàÎã§."
      ],
      "metadata": {
        "id": "PHOoPaU2OkGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üêπ sent_tokenize()\n",
        "\n",
        "* I am not happy with results with conjunctions, post-numbers, etc. "
      ],
      "metadata": {
        "id": "eZ37_yhXOmFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk \n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sentences = \"I'm actively looking for Ph.D. students. \\\n",
        "and you are a Ph.D student. \\\n",
        "Visit IP 192.168.56.31 \\\n",
        "and send the results to my email account. \\\n",
        "It's email@gmail.com.\"\n",
        "\n",
        "sentence = sent_tokenize(sentences)\n",
        "print('Î¨∏Ïû• ÌÜ†ÌÅ∞Ìôî: %s' %sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv8s_PN-OlG_",
        "outputId": "c2bd4eaf-1aa9-4786-ddac-19d08768c628"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Î¨∏Ïû• ÌÜ†ÌÅ∞Ìôî: [\"I'm actively looking for Ph.D. students.\", 'and you are a Ph.D student.', 'Visit IP 192.168.56.31 and send the results to my email account.', \"It's email@gmail.com.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [For Tokenization and POS in Korean, visit Wikidocs for further information](https://wikidocs.net/21698) \n",
        "* Sentence tokenizer for Korean: kss package"
      ],
      "metadata": {
        "id": "xGOApXRERwUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kss\n",
        "import kss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHP16yVbR4Z4",
        "outputId": "fd980c6a-5caa-42a8-b3e6-b4fce8ba0db9"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kss\n",
            "  Using cached kss-4.5.3.tar.gz (78 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting emoji==1.2.0 (from kss)\n",
            "  Using cached emoji-1.2.0-py3-none-any.whl (131 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from kss) (2022.10.31)\n",
            "Collecting pecab (from kss)\n",
            "  Using cached pecab-1.0.8.tar.gz (26.4 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from kss) (3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (1.22.4)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (9.0.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from pecab->kss) (7.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (23.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (1.1.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->pecab->kss) (2.0.1)\n",
            "Building wheels for collected packages: kss, pecab\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-4.5.3-py3-none-any.whl size=54258 sha256=07ac04a15bddf59d3dec45404a9d84d4124cba5b422f3945b1aaf434a9ac6e18\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/9e/a3/5b09e3f14722fa0d77f47fe840668d426760023bdd11b0fbd9\n",
            "  Building wheel for pecab (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pecab: filename=pecab-1.0.8-py3-none-any.whl size=26646666 sha256=42d490a00b3b299cfa6e398b2a3c0f1c4fcdf0c8edc88db190d4d5b49fdba482\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/6f/b4/ab61b8863d7d8b1409def8ae31adcaa089fa91b8d022ec309d\n",
            "Successfully built kss pecab\n",
            "Installing collected packages: emoji, pecab, kss\n",
            "Successfully installed emoji-1.2.0 kss-4.5.3 pecab-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "korfl = ('/content/sample_data/Exercise/Squall.txt', 'rt')\n",
        "korfl.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "RQez4XkpTjSE",
        "outputId": "b7347f0a-a227-4baa-8923-e20b989573e9"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-178-09b879893adf>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mkorfl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'/content/sample_data/Exercise/Squall.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkorfl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'read'"
          ]
        }
      ]
    }
  ]
}