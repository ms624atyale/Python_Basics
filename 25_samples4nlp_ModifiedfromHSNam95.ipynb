{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Copy of nlp.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms624atyale/Python_Basics/blob/main/25_samples4nlp_ModifiedfromHSNam95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltUO0sYwyGfU"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di6xZ08xsgO7"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbHEyyNHntcZ"
      },
      "source": [
        "txt = 'Here’s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. \\\n",
        "The ones who see things differently — they’re not fond of rules. \\\n",
        "You can quote them, disagree with them, glorify or vilify them, \\\n",
        "but the only thing you can’t do is ignore them because they change things. \\\n",
        "They push the human race forward, and while some may see them as the crazy ones, we see genius, \\\n",
        "because the ones who are crazy enough to think that they can change the world, are the ones who do.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(txt)"
      ],
      "metadata": {
        "id": "2gn1acvcFo4I",
        "outputId": "fac90fbf-577d-4a90-ae1e-1b467e70d12f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here’s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. The ones who see things differently — they’re not fond of rules. You can quote them, disagree with them, glorify or vilify them, but the only thing you can’t do is ignore them because they change things. They push the human race forward, and while some may see them as the crazy ones, we see genius, because the ones who are crazy enough to think that they can change the world, are the ones who do.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('/content/sample_data/exercise/crime_punishment_sample.txt', 'rt') #Don't forget to load your txt file!\n",
        "text = file.read().replace(\"\\n\", \" \")\n",
        "file.close()"
      ],
      "metadata": {
        "id": "btgs9Nt-2Yj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45jV2UYs1GEC",
        "outputId": "474e02a0-8aee-44d9-bdc7-a6af31942ab9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "txt.split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Here’s',\n",
              " 'to',\n",
              " 'the',\n",
              " 'crazy',\n",
              " 'ones,',\n",
              " 'the',\n",
              " 'misfits,',\n",
              " 'the',\n",
              " 'rebels,',\n",
              " 'the',\n",
              " 'troublemakers,',\n",
              " 'the',\n",
              " 'round',\n",
              " 'pegs',\n",
              " 'in',\n",
              " 'the',\n",
              " 'square',\n",
              " 'holes.',\n",
              " 'The',\n",
              " 'ones',\n",
              " 'who',\n",
              " 'see',\n",
              " 'things',\n",
              " 'differently',\n",
              " '—',\n",
              " 'they’re',\n",
              " 'not',\n",
              " 'fond',\n",
              " 'of',\n",
              " 'rules.',\n",
              " 'You',\n",
              " 'can',\n",
              " 'quote',\n",
              " 'them,',\n",
              " 'disagree',\n",
              " 'with',\n",
              " 'them,',\n",
              " 'glorify',\n",
              " 'or',\n",
              " 'vilify',\n",
              " 'them,',\n",
              " 'but',\n",
              " 'the',\n",
              " 'only',\n",
              " 'thing',\n",
              " 'you',\n",
              " 'can’t',\n",
              " 'do',\n",
              " 'is',\n",
              " 'ignore',\n",
              " 'them',\n",
              " 'because',\n",
              " 'they',\n",
              " 'change',\n",
              " 'things.',\n",
              " 'They',\n",
              " 'push',\n",
              " 'the',\n",
              " 'human',\n",
              " 'race',\n",
              " 'forward,',\n",
              " 'and',\n",
              " 'while',\n",
              " 'some',\n",
              " 'may',\n",
              " 'see',\n",
              " 'them',\n",
              " 'as',\n",
              " 'the',\n",
              " 'crazy',\n",
              " 'ones,',\n",
              " 'we',\n",
              " 'see',\n",
              " 'genius,',\n",
              " 'because',\n",
              " 'the',\n",
              " 'ones',\n",
              " 'who',\n",
              " 'are',\n",
              " 'crazy',\n",
              " 'enough',\n",
              " 'to',\n",
              " 'think',\n",
              " 'that',\n",
              " 'they',\n",
              " 'can',\n",
              " 'change',\n",
              " 'the',\n",
              " 'world,',\n",
              " 'are',\n",
              " 'the',\n",
              " 'ones',\n",
              " 'who',\n",
              " 'do.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFR-cRaahTPy",
        "outputId": "23878c2e-3f81-44b7-e1fa-c271dc6549d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "' '.join(txt.split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here’s to the crazy ones, the misfits, the rebels, the troublemakers, the round pegs in the square holes. The ones who see things differently — they’re not fond of rules. You can quote them, disagree with them, glorify or vilify them, but the only thing you can’t do is ignore them because they change things. They push the human race forward, and while some may see them as the crazy ones, we see genius, because the ones who are crazy enough to think that they can change the world, are the ones who do.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK (Natural Language Toolkit)** is a Python library and package for working with human language data, particularly for natural language processing (NLP) and linguistic analysis. It provides tools, resources, and libraries for tasks such as text processing, tokenization, part-of-speech tagging, parsing, and much more.\n",
        "\n",
        "In summary:\n",
        "\n",
        "**NLTK is a library**: NLTK is primarily a Python library that offers various functionalities and tools for working with text and language data.\n",
        "\n",
        "**NLTK is a package**: NLTK is distributed as a Python package, and you can install it using package management tools like **pip**. For example, you can install NLTK using pip install nltk.\n",
        "\n",
        "NLTK is a collection of modules: Within the NLTK library, you'll find various modules and sub-packages that cater to different aspects of natural language processing and text analysis. These modules contain functions, data, and resources to support a wide range of linguistic and text-related tasks.\n",
        "\n",
        "In essence, NLTK is a comprehensive resource for NLP in Python, and it is commonly used by researchers, developers, and linguists for various text analysis and language processing tasks."
      ],
      "metadata": {
        "id": "kohBS0yeiUvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "lGpNCLnQHGvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🆘 What is PunktSentenceTokenizer?\n",
        "* [For more information read the original article](https://www.askpython.com/python-modules/nltk-punkt)\n",
        "\n",
        "In NLTK, PUNKT is an <font color = 'brown'> **unsupervised trainable model**</font>, which means it can be **trained on unlabeled data** (Data that has not been tagged with information identifying its characteristics, properties, or categories is referred to as unlabeled data.)\n",
        "\n",
        "It generates a list of sentences from a text by developing a model for **words that start sentences, prepositional phrases, and abbreviations** using an unsupervised technique. Without first being put to use, it has to be trained on a sizable amount of plaintext in the intended language."
      ],
      "metadata": {
        "id": "PaMm4fnFe67P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#⛔ <font color = 'red'> Q: module 따로 불러들여야 하지 않나? Delete later."
      ],
      "metadata": {
        "id": "_v9SOJjRghF-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq_lPZMHntcb",
        "outputId": "89567312-b687-439f-d0dc-edc45c85b90e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "words = word_tokenize(txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#⛔ <font color = 'red'> Q:RE \"[\\w]+\"?를 적용해서 어떤 결과가 나오는지 이해X. Delete later."
      ],
      "metadata": {
        "id": "piEwdVuig_U_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC1fe7nWF6wN",
        "outputId": "f013a16c-7164-4ee3-e6c6-e1abe6940fa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "retokenize = RegexpTokenizer(\"[\\w]+\")\n",
        "words = retokenize.tokenize(txt)\n",
        "words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Here',\n",
              " 's',\n",
              " 'to',\n",
              " 'the',\n",
              " 'crazy',\n",
              " 'ones',\n",
              " 'the',\n",
              " 'misfits',\n",
              " 'the',\n",
              " 'rebels',\n",
              " 'the',\n",
              " 'troublemakers',\n",
              " 'the',\n",
              " 'round',\n",
              " 'pegs',\n",
              " 'in',\n",
              " 'the',\n",
              " 'square',\n",
              " 'holes',\n",
              " 'The',\n",
              " 'ones',\n",
              " 'who',\n",
              " 'see',\n",
              " 'things',\n",
              " 'differently',\n",
              " 'they',\n",
              " 're',\n",
              " 'not',\n",
              " 'fond',\n",
              " 'of',\n",
              " 'rules',\n",
              " 'You',\n",
              " 'can',\n",
              " 'quote',\n",
              " 'them',\n",
              " 'disagree',\n",
              " 'with',\n",
              " 'them',\n",
              " 'glorify',\n",
              " 'or',\n",
              " 'vilify',\n",
              " 'them',\n",
              " 'but',\n",
              " 'the',\n",
              " 'only',\n",
              " 'thing',\n",
              " 'you',\n",
              " 'can',\n",
              " 't',\n",
              " 'do',\n",
              " 'is',\n",
              " 'ignore',\n",
              " 'them',\n",
              " 'because',\n",
              " 'they',\n",
              " 'change',\n",
              " 'things',\n",
              " 'They',\n",
              " 'push',\n",
              " 'the',\n",
              " 'human',\n",
              " 'race',\n",
              " 'forward',\n",
              " 'and',\n",
              " 'while',\n",
              " 'some',\n",
              " 'may',\n",
              " 'see',\n",
              " 'them',\n",
              " 'as',\n",
              " 'the',\n",
              " 'crazy',\n",
              " 'ones',\n",
              " 'we',\n",
              " 'see',\n",
              " 'genius',\n",
              " 'because',\n",
              " 'the',\n",
              " 'ones',\n",
              " 'who',\n",
              " 'are',\n",
              " 'crazy',\n",
              " 'enough',\n",
              " 'to',\n",
              " 'think',\n",
              " 'that',\n",
              " 'they',\n",
              " 'can',\n",
              " 'change',\n",
              " 'the',\n",
              " 'world',\n",
              " 'are',\n",
              " 'the',\n",
              " 'ones',\n",
              " 'who',\n",
              " 'do']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(words)"
      ],
      "metadata": {
        "id": "wxVkzTpOHO29",
        "outputId": "9fb1f853-b150-4f84-8929-a02dad599acc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Here', 's', 'to', 'the', 'crazy', 'ones', 'the', 'misfits', 'the', 'rebels', 'the', 'troublemakers', 'the', 'round', 'pegs', 'in', 'the', 'square', 'holes', 'The', 'ones', 'who', 'see', 'things', 'differently', 'they', 're', 'not', 'fond', 'of', 'rules', 'You', 'can', 'quote', 'them', 'disagree', 'with', 'them', 'glorify', 'or', 'vilify', 'them', 'but', 'the', 'only', 'thing', 'you', 'can', 't', 'do', 'is', 'ignore', 'them', 'because', 'they', 'change', 'things', 'They', 'push', 'the', 'human', 'race', 'forward', 'and', 'while', 'some', 'may', 'see', 'them', 'as', 'the', 'crazy', 'ones', 'we', 'see', 'genius', 'because', 'the', 'ones', 'who', 'are', 'crazy', 'enough', 'to', 'think', 'that', 'they', 'can', 'change', 'the', 'world', 'are', 'the', 'ones', 'who', 'do']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc42Plwx56YS"
      },
      "source": [
        "### Normalization  \n",
        "Stemming: am → am, the going → the go, having → hav  \n",
        "Lemmatization: am → be, the going → the going, having → have"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#⛔ <font color = 'red'> Q:RE stem 은 1줄에서 모듈 같은데, 3줄에서 stem(w)라고 사용된 것 보면 함수 같은데... 이런 것들은 학생들에게 어떻게 설명해야 하나?  Delete later."
      ],
      "metadata": {
        "id": "TOlfnt7hhj_1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsFfoAr259Fs"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "[stemmer.stem(w) for w in words] # words 로 list comprehension. 이 워즈에서 하나하나씩 루프를 도는데, 단어를 w 에 담아서, 앞에 stemmer.stem () 함수가 실행된다. 어떤 함수는 어떤 장/단점이 있구나."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkbgNiPd8BdL"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "[stemmer.stem(w) for w in words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "👀 **WordNet** is just another NLTK corpus reader, and can be imported like this:\n",
        "\n",
        "[For more information, you can read the original article](https://www.nltk.org/howto/wordnet.html)\n",
        ">> from nltk.corpus import wordnet as wn\n",
        "\n",
        "Look up a word using synsets(); this function has an optional pos argument which lets you constrain the part of speech of the word:\n",
        "\n",
        ">> wn.synsets('dog', pos=wn.VERB)\n",
        "\n",
        ">> [Synset('chase.v.01')]\n"
      ],
      "metadata": {
        "id": "7ZGPXKWHjfvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#⛔ <font color = 'red'> Q: WordNetLemmatizer와는 다르게 download 함수는 기본함수라서 바로 사용 가능. 예시: print().\n",
        "\n",
        "#⛔ <font color = 'red'> Q: 찾다가 봤는데, function1.function2() 이렇게 사용하는 경우도 흔한지? 이것과 function1(function2()) 와는 어떻게 다른지... 아래 lemmatizer.lemmatize(w) Delete later"
      ],
      "metadata": {
        "id": "nNlyHyuSiTkB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIIh5pYd8f74"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer # 활용을 복원하여 단어를 추출해줌.\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "[lemmatizer.lemmatize(w) for w in words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Stopwords** 불용어\n",
        "\n",
        "[For more information, read the original article](https://pythonspot.com/nltk-stop-words/)\n",
        "\n",
        "* Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc. Such words are already captured this in corpus named corpus. We first download it to our python environment.\n",
        "\n",
        "* Stop words are common words like ‘the’, ‘and’, ‘I’, etc. that are very frequent in text, and so don’t convey insights into the specific topic of a document. We can remove these stop words from the text in a given corpus to clean up the data, and identify words that are more rare and potentially more relevant to what we’re interested in.\n",
        "\n",
        "* Text may contain stop words like ‘the’, ‘is’, ‘are’. Stop words can be filtered from the text to be processed. There is no universal list of stop words in nlp research, however the nltk module contains a list of stop words."
      ],
      "metadata": {
        "id": "Y49DTFpdmehh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPgIzrjm8_1N"
      },
      "source": [
        "### Stopword"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk #🍄 package or module?"
      ],
      "metadata": {
        "id": "zOljqQEKIqXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⛔ Q: 함수 연속 사용에 규칙은 뭔가? function1(function(2)); function1.function2()"
      ],
      "metadata": {
        "id": "Nw9Z2rE4q3bv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdM2FaN8ntcc",
        "outputId": "5a031bb0-ad02-46b3-cca9-8524ba239264",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords') #필요한 모듈??라서 따로 다운로드 함.\n",
        "words = [w for w in words if not w in stopwords.words('english')] #🍄 codeline 해석 못함X. words 는 변수인데, 왜 함수()처럼 쓰이지?\n",
        "print(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Here', 'crazy', 'ones', 'misfits', 'rebels', 'troublemakers', 'round', 'pegs', 'square', 'holes', 'The', 'ones', 'see', 'things', 'differently', 'fond', 'rules', 'You', 'quote', 'disagree', 'glorify', 'vilify', 'thing', 'ignore', 'change', 'things', 'They', 'push', 'human', 'race', 'forward', 'may', 'see', 'crazy', 'ones', 'see', 'genius', 'ones', 'crazy', 'enough', 'think', 'change', 'world', 'ones']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmwXTL0UA5aw"
      },
      "source": [
        "### Collocation, Concordance (To do list as of May 18, 2023)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fll4ygxNA3OJ"
      },
      "source": [
        "nltk.download('gutenberg')\n",
        "text = nltk.corpus.gutenberg.raw('austen-emma.txt') #Package.Module.gutenberg자료.raw()함수\n",
        "words = retokenize.tokenize(text)#?? [\\w]+ RE을 적용한 결과를 할당한 retokenize 변수에 위의 text변수를 토큰화하는 함수 사용"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TuKfbhCX0-1m"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqVXlhIrAtmf"
      },
      "source": [
        "nltk.Text(words).collocations()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq0wiutwA_au"
      },
      "source": [
        "nltk.Text(words).concordance('Emma', 79, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIAIhXvP_BjU"
      },
      "source": [
        "nltk.Text(words).dispersion_plot([\"Emma\", \"Knightley\", \"Frank\", \"Jane\", \"Harriet\", \"Robert\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWYZOFxq_ex2"
      },
      "source": [
        "nltk.Text(words).similar(\"Emma\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZihiVSBK_vy7"
      },
      "source": [
        "nltk.Text(words).common_contexts([\"Emma\", \"she\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8TrCE14vGcT"
      },
      "source": [
        "### Frequency distribution, Frequency plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdY3m6zSBHic"
      },
      "source": [
        "fd = nltk.FreqDist(words).most_common(20)\n",
        "fd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tpZThNV-ftv"
      },
      "source": [
        "nltk.Text(words).plot(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSOSzIovvKvE"
      },
      "source": [
        "### Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIcAOAvqntce"
      },
      "source": [
        "nltk.download('words')\n",
        "nltk.corpus.words.words('en')[-20:-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjAy_Ju7ntce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30d2cf84-b481-4e7b-82fa-81bcf0ca4555"
      },
      "source": [
        "len(nltk.corpus.words.words('en'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235886"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrVGVc0X9j7r"
      },
      "source": [
        "### Regular expression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQKgoQFI_cG-"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U2pS-NL9p38"
      },
      "source": [
        "'''       Basic Regular Expression Meta-Characters, Including Wildcards, Ranges and Closures\n",
        "\n",
        ".\t        Wildcard, matches any character\n",
        "^abc\t    Matches some pattern abc at the start of a string\n",
        "abc$\t    Matches some pattern abc at the end of a string\n",
        "[abc]\t    Matches one of a set of characters\n",
        "[^abc]    Matches anything but a set of characters\n",
        "[A-Z0-9]\tMatches one of a range of characters\n",
        "ed|ing|s\tMatches one of the specified strings (disjunction)\n",
        "*\t        Zero or more of previous item, e.g. a*, [a-z]* (also known as Kleene Closure)\n",
        "+\t        One or more of previous item, e.g. a+, [a-z]+\n",
        "?\t        Zero or one of the previous item (i.e. optional), e.g. a?, [a-z]?\n",
        "{n}\t      Exactly n repeats where n is a non-negative integer\n",
        "{n,}\t    At least n repeats\n",
        "{,n}\t    No more than n repeats\n",
        "{m,n}\t    At least m and no more than n repeats\n",
        "a(b|c)+\t  Parentheses that indicate the scope of the operators\n",
        "(...)     Matches whatever regular expression is inside the parentheses\n",
        "\\d\n",
        "Matches any decimal digit; this is equivalent to the class [0-9].\n",
        "\\D\n",
        "Matches any non-digit character; this is equivalent to the class [^0-9].\n",
        "\\s\n",
        "Matches any whitespace character; this is equivalent to the class [ \\t\\n\\r\\f\\v].\n",
        "\\S\n",
        "Matches any non-whitespace character; this is equivalent to the class [^ \\t\\n\\r\\f\\v].\n",
        "\\w\n",
        "Matches any alphanumeric character; this is equivalent to the class [a-zA-Z0-9_].\n",
        "\\W\n",
        "Matches any non-alphanumeric character; this is equivalent to the class [^a-zA-Z0-9_].\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp3_Dm9Q_tNQ"
      },
      "source": [
        "engdict = nltk.corpus.words.words('en')\n",
        "\n",
        "result = [w for w in engdict if re.search('ed$', w)]\n",
        "# result = [w for w in engdict if re.search('^..j..t..$', w)]\n",
        "# result = [w for w in engdict if re.search('^[ghi][mno][jlk][def]$', w)]\n",
        "# result = [w for w in engdict if re.search('^[ah]+$', w)][:10]\n",
        "print(result[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Treebank** in the NLTK (Natural Language Toolkit) package refers to a collection of parsed and annotated sentences from various languages, primarily English. It is a valuable linguistic resource commonly used for natural language processing (NLP) research, especially in the context of syntactic and grammatical analysis.\n",
        "\n",
        "Here are some key aspects of the Treebank in the NLTK package:\n",
        "\n",
        "- Parsed Sentences: The Treebank consists of sentences that have been manually parsed or annotated to represent their syntactic structure. These annotations include part-of-speech tags, phrase structure trees, and dependency trees.\n",
        "\n",
        "- Penn Treebank Format: The Treebank in NLTK often uses the Penn Treebank format for representing syntactic structures. In this format, sentences are represented as nested tree structures, where words are labeled with their part-of-speech tags, and phrases are organized hierarchically.\n",
        "\n",
        "- Annotations: Each word in a sentence is tagged with its part of speech (e.g., noun, verb, adjective) and may have additional syntactic annotations like noun phrases, verb phrases, and clauses. These annotations help NLP researchers and practitioners analyze sentence structure and grammar.\n",
        "\n",
        "- Applications: The Treebank is used for a wide range of NLP tasks, including:\n",
        "\n",
        "  - Developing and evaluating syntactic parsers.\n",
        "  - Training and evaluating part-of-speech taggers.\n",
        "  - Studying linguistic phenomena and syntactic patterns.\n",
        "  - Building and evaluating machine learning models for language understanding.\n",
        "\n",
        "- Corpus Diversity: The NLTK Treebank contains a diverse set of text genres, including news articles, fiction, academic papers, and more. This diversity helps ensure that NLP research and models trained on the Treebank are applicable to various text types.\n",
        "\n",
        "- Availability: The NLTK Treebank is readily accessible to researchers and developers through the NLTK library in Python. It can be easily downloaded and used for research, experimentation, and model training.\n",
        "\n",
        "In NLTK (Natural Language Toolkit), the term <font color = 'red'> **Treebank**</font> refers to a specific dataset or corpus that contains sentences from various sources, which have been syntactically parsed and annotated. It is not a standalone module; instead, it is one of the many corpora (plural of corpus) available within NLTK.\n",
        "\n",
        "Here's a brief overview:\n",
        "\n",
        "- Corpus in NLTK: NLTK provides a wide range of corpora, which are collections of text data that are typically used for linguistic analysis, research, and natural language processing tasks. These corpora cover various languages and domains.\n",
        "\n",
        "- Treebank Corpus: The Treebank corpus within NLTK contains sentences from the Penn Treebank Project, which is a well-known linguistic resource. These sentences have been parsed to represent their syntactic structure, and they include part-of-speech tags, phrase structure trees, and sometimes dependency trees.\n",
        "\n",
        "- Usage: You can access the Treebank corpus and its sentences using NLTK's corpus reader. It is often used for tasks related to syntactic parsing, part-of-speech tagging, and linguistic research.\n",
        "\n",
        "In summary, the Treebank in NLTK is not a standalone module but rather a specific corpus that you can access and use within NLTK. It contains linguistically annotated sentences and is a valuable resource for various natural language processing and linguistic analysis tasks.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iy8vw_G1dngt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1a5mQYj4hwn"
      },
      "source": [
        "nltk.download('treebank')\n",
        "wsj = nltk.corpus.treebank.words()\n",
        "\n",
        "result = [w for w in wsj if re.search('(ed|ing)$', w)]\n",
        "# result = [w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)]\n",
        "# result = [w for w in wsj if re.search('^[A-Z]+\\$$', w)]\n",
        "# result = [w for w in wsj if re.search('^[0-9]{4}$', w)]\n",
        "# result = [w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)]\n",
        "# result = [w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]\n",
        "\n",
        "result = sorted(set(result))\n",
        "print(result[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyIIqwosCRZa"
      },
      "source": [
        "### Extract information (pos tag, named entity)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VBiObftCVwH"
      },
      "source": [
        "sent = \"I am Jhon from America and would like to go to Starbuck\"\n",
        "words = nltk.word_tokenize(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2HuyzFWCe3U"
      },
      "source": [
        "'''\n",
        "POS tag list:\n",
        "\n",
        "CC\tcoordinating conjunction\n",
        "CD\tcardinal digit\n",
        "DT\tdeterminer\n",
        "EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
        "FW\tforeign word\n",
        "IN\tpreposition/subordinating conjunction\n",
        "JJ\tadjective\t'big'\n",
        "JJR\tadjective, comparative\t'bigger'\n",
        "JJS\tadjective, superlative\t'biggest'\n",
        "LS\tlist marker\t1)\n",
        "MD\tmodal\tcould, will\n",
        "NN\tnoun, singular 'desk'\n",
        "NNS\tnoun plural\t'desks'\n",
        "NNP\tproper noun, singular\t'Harrison'\n",
        "NNPS\tproper noun, plural\t'Americans'\n",
        "PDT\tpredeterminer\t'all the kids'\n",
        "POS\tpossessive ending\tparent's\n",
        "PRP\tpersonal pronoun\tI, he, she\n",
        "PRP$\tpossessive pronoun\tmy, his, hers\n",
        "RB\tadverb\tvery, silently,\n",
        "RBR\tadverb, comparative\tbetter\n",
        "RBS\tadverb, superlative\tbest\n",
        "RP\tparticle\tgive up\n",
        "TO\tto\tgo 'to' the store.\n",
        "UH\tinterjection\terrrrrrrrm\n",
        "VB\tverb, base form\ttake\n",
        "VBD\tverb, past tense\ttook\n",
        "VBG\tverb, gerund/present participle\ttaking\n",
        "VBN\tverb, past participle\ttaken\n",
        "VBP\tverb, sing. present, non-3d\ttake\n",
        "VBZ\tverb, 3rd person sing. present\ttakes\n",
        "WDT\twh-determiner\twhich\n",
        "WP\twh-pronoun\twho, what\n",
        "WP$\tpossessive wh-pronoun\twhose\n",
        "WRB\twh-abverb\twhere, when\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwKdu36WCewv"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos = nltk.pos_tag(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnjGT1HpClE0"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "NE = nltk.ne_chunk(pos)\n",
        "# common Entity types: ORGANIZATION, PERSON, LOCATION, DATE, TIME, MONEY, and GPE (geo-political entity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jt9DEIZ4lXQF"
      },
      "source": [
        "### Wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jypxOnw9hoyZ"
      },
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text = nltk.corpus.gutenberg.raw('bible-kjv.txt')\n",
        "\n",
        "wc = WordCloud().generate(text)\n",
        "plt.imshow(wc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6xv5ClAl5xk"
      },
      "source": [
        "stopwords = set(STOPWORDS)\n",
        "stopwords.add('unto')\n",
        "wc = WordCloud(stopwords = stopwords).generate(text)\n",
        "plt.imshow(wc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}