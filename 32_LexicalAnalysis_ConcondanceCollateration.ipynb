{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LexicalAnalysis_ConcondanceCollateration.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNwykRoQQ+6YZhkZZ+FDIKQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ms624atyale/Python_Basics/blob/main/32_LexicalAnalysis_ConcondanceCollateration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ”§ ğŸ”¨ ğŸ‘€ Under Construction\n",
        "#ğŸ“š ğŸ‘€ Essential Steps before Text Analysis\n",
        "\n",
        "## ğŸ¹ ğŸ¾ [Text Corpus <font size='1.8'>ì½”í¼ìŠ¤/ë§ë­‰ì¹˜</font>](https://en.wikipedia.org/wiki/Text_corpus)\n",
        "- In linguistics, a corpus (plural corpora) or text corpus is a language resource consisting of a large and structured set of texts (nowadays usually electronically stored and processed). In corpus linguistics, they are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory.\n",
        "\n",
        "- The **corpus-toolkit** package grew out of courses in corpus linguistics and learner corpus research. The toolkit attempts to balance simplicity of use, broad application, and scalability. Common corpus analyses such as the <font color = 'red'>_calculation of word and n-gram frequency and range, keyness, and collocation_</font> are included. In addition, more advanced analyses such as the identification of <font color = 'red'>_dependency bigrams (e.g., verb-direct object combinations) and their frequency, range, and strength of association_</font>  are also included.(https://pypi.org/project/corpus-toolkit/)\n",
        "\n",
        "Some conditions should be fulfilled if you want to conduct corpus-related analysis.\n",
        "\n",
        ">1. Read and write a file using an operating system package.\n",
        ">2. ğŸ†˜ import the **[os](https://docs.python.org/3/library/os.html)** module.\n",
        "\n",
        "\n",
        ">3. Text files you want to analyze (e.g., url(uniform resource locator) with html document, text files under the Files dicrectory of Google Colab).\n",
        ">4. Text â¡ï¸ Words: **Tokenization**\n",
        ">5. Words with the conjugation, inflection, derivation process â†”ï¸ Words sorted by grouping inflected or variant forms of the same word (i.e., **lemmatization**)\n",
        ">6. POS (part of speech (e.g., word-grammatical category pairs))\n",
        ">7. ğŸ†˜ Install **corpus-toolkit** and **nltk**(natural language tool kit) packages.\n",
        "\n"
      ],
      "metadata": {
        "id": "M7afPLwh-bRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collocation (ì—°ì–´, ì–´êµ¬ ê²°í•©): ì—°ì–´ ë˜ëŠ” ì–´êµ¬ ê²°í•©ì€ íŠ¹ì •í•œ ë‹¨ì–´ë‚˜ ì–´êµ¬ê°€ ìì£¼ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” ì–¸ì–´ì  ê²°í•©ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ê²ƒì€ íŠ¹ì •í•œ ë§ë“¤ì´ ìì—°ìŠ¤ëŸ½ê²Œ í•¨ê»˜ ì‚¬ìš©ë˜ëŠ” ê²½í–¥ì´ ìˆëŠ” ì–´êµ¬ ë˜ëŠ” ë‹¨ì–´ ì¡°í•©ì„ ê°€ë¦¬í‚µë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ \"ê°•í•œ ì»¤í”¼,\" \"ë°ì€ íƒœì–‘,\" ë˜ëŠ” \"í–‰ë³µí•œ ì•„ì´ë“¤\"ì€ ì—°ì–´ ë˜ëŠ” ì–´êµ¬ ê²°í•©ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤.\n",
        "\n",
        "Concordance (ì—°ì–´ ë¶„ì„): ì—°ì–´ ë¶„ì„ì€ ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë‚´ì—ì„œ íŠ¹ì •í•œ ë‹¨ì–´ë‚˜ ì–´êµ¬ì˜ ì¶œí˜„ì„ ì°¾ì•„ ê·¸ ì£¼ë³€ ë¬¸ë§¥ì„ í•¨ê»˜ í‘œì‹œí•˜ëŠ” í…ìŠ¤íŠ¸ ë¶„ì„ ê¸°ìˆ ì…ë‹ˆë‹¤. ì´ëŠ” íŠ¹ì •í•œ ë‹¨ì–´ë‚˜ ì–´êµ¬ê°€ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ë¥¼ ì´í•´í•˜ê¸° ìœ„í•œ ê²ƒìœ¼ë¡œ, ë¬¸ë²•ì , ì˜ë¯¸ì  ê´€ë ¨ì„±ì„ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.\n",
        "\n",
        "Import Miran Kim's concordance"
      ],
      "metadata": {
        "id": "6P3Ry3VIILxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ“Œ Download the os module\n",
        "import os"
      ],
      "metadata": {
        "id": "J-ZgsoO2LnYg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ“Œ Make a new working directory as \"txtdata\". ğŸ“ <Module name: os> <function: mkdir>\n",
        "\n",
        "os.mkdir(\"txtfolder\")"
      ],
      "metadata": {
        "id": "Nt17SlKpUq_f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JBz4z96Z-YV5",
        "outputId": "080f91cc-f493-458f-c061-62c7a045edb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting corpus-toolkit\n",
            "  Downloading corpus_toolkit-0.32-py3-none-any.whl (1.7 MB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: corpus-toolkit\n",
            "Successfully installed corpus-toolkit-0.32\n"
          ]
        }
      ],
      "source": [
        "#@markdown ğŸ“Œ Download the corpus-toolkit package\n",
        "!pip install corpus-toolkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lexical-diversity\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bWvAY-gxSf9",
        "outputId": "a00649aa-22c5-442c-8446-2d509fd8cbae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lexical-diversity\n",
            "  Downloading lexical_diversity-0.1.1-py3-none-any.whl (117 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/117.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m112.6/117.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.8/117.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lexical-diversity\n",
            "Successfully installed lexical-diversity-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "cHNMVuCExYrU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ“Œ Get working directory. <code line: print working directory>\n",
        "%pwd"
      ],
      "metadata": {
        "id": "6YiwfB3fVfQy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8b3c25c5-c082-4cfe-a54d-0c4f39cfae6c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ms624atyale/Data_Class"
      ],
      "metadata": {
        "id": "MxxXq59qJdVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ“Œ Open a txt file. <Use a set of duble quotation marks \"\" and assign the url address as the _url_ variable>\n",
        "\n",
        "url=\"https://raw.githubusercontent.com/ms624atyale/Data_Class/main/PG%20Aesop%20Collocation%20Concordance/Test1.txt\"\n",
        "\n",
        "\n",
        "#os.system(\"curl \" + url + \" > test1.txt\") #This generates a txt file under the txtfolder directory and moves the whole text of the url to the txt file (e.g., foxtail.txt).\n",
        "\n",
        "file=open('/content/txtfolder/Test1.txt','rt')\n",
        "#file = open(\"test1.txt\")\n",
        "text = file.read().replace(\"\\n\", \" \") #Replace line with a space.\n",
        "file.close() #Close the file you have been working on.\n",
        "\n",
        "#@markdown ğŸ“ When you see crimepunish.txt under the Files directory, move it under the txtfolder folder you've created by drag & drop."
      ],
      "metadata": {
        "id": "cn5UjS8xWUpN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ“Œ i) Tokenize your text and ii) Get frequency.\n",
        "\n",
        "from corpus_toolkit import corpus_tools as ct\n",
        "txt = ct.ldcorpus(\"txtfolder\") #load and read the 'txtfolder' folder (cf., NOT \"foxtail.txt\")\n",
        "tok_corp = ct.tokenize(txt) #tokenize corpus - by default this lemmatizes as well\n",
        "txt_freq = ct.frequency(txt) #creates a frequency dictionary\n",
        "ct.head(txt_freq, hits = 20) #print top 20 items"
      ],
      "metadata": {
        "id": "u-Ph5SUx79UP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e4f890f-c568-4bdd-ec22-158d927c198d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing Test1.txt (1 of 1 files)\n",
            "e\t79\n",
            "h\t47\n",
            "t\t45\n",
            "a\t39\n",
            "i\t38\n",
            "n\t36\n",
            "o\t35\n",
            "l\t33\n",
            "r\t31\n",
            "s\t30\n",
            "d\t22\n",
            "\n",
            "\t18\n",
            "w\t18\n",
            "g\t17\n",
            "f\t14\n",
            "m\t13\n",
            "c\t12\n",
            ".\t10\n",
            "k\t8\n",
            "p\t8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from corpus_toolkit import corpus_tools as ct\n",
        "\n",
        "txt = ct.ldcorpus(\"txtfolder\") #load and read the 'txtfolder' folder (cf., NOT \"foxtail.txt\")\n",
        "\n",
        "from lexical_diversity import lex_div as ld\n",
        "import re\n",
        "\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b') #Getting rid of Stopwords of 1~2 spellings. Regular expression\n",
        "txt = shortword.sub('',txt)\n",
        "\n",
        "tok_corp = ct.tokenize(txt) #tokenize corpus - by default this lemmatizes as well\n",
        "txt_freq = ct.frequency(txt) #creates a frequency dictionary\n",
        "ct.head(txt_freq, hits = 20) #print top 20 items"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "KP-EzJHeuxwM",
        "outputId": "46936423-6b99-406e-c500-b598ed119069"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-431b19729bc5>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mshortword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\W*\\b\\w{1,2}\\b'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Getting rid of Stopwords of 1~2 spellings. Regular expression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshortword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtok_corp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#tokenize corpus - by default this lemmatizes as well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ“Œ Tagging (i.e., associating each token with a grammatical category (e.g., mountain - N) )\n",
        "ct.write_corpus(\"tagged_txt\",ct.tag(ct.ldcorpus(\"txtfolder\")))"
      ],
      "metadata": {
        "id": "EZ7XatV0ZpMh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f927971-8286-4baf-a96a-a41ee4545ccc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing files to existing folder\n",
            "Processing txtdata.txt (1 of 1 files)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ“Œ Get frequency of your tagged tokens. 'hits=10' means you want to get the top 10 words.\n",
        "\n",
        "tagged_freq = ct.frequency(ct.reload(\"tagged_txt\"))\n",
        "ct.head(tagged_freq, hits = 200)"
      ],
      "metadata": {
        "id": "TxTrpzkEaXiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28fcef07-8ea0-47a4-c7a1-b3ff679e0219"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 1.txt (1 of 2 files)\n",
            "Processing 2.txt (2 of 2 files)\n",
            "the_DET\t4344\n",
            "and_CCONJ\t1798\n",
            "be_AUX\t1666\n",
            "a_DET\t1624\n",
            "he_PRON\t1522\n",
            "to_PART\t1138\n",
            "of_ADP\t1012\n",
            "his_PRON\t742\n",
            "in_ADP\t710\n",
            "you_PRON\t626\n",
            "they_PRON\t574\n",
            "I_PRON\t560\n",
            "it_PRON\t530\n",
            "not_PART\t512\n",
            "to_ADP\t454\n",
            "have_AUX\t438\n",
            "with_ADP\t412\n",
            "but_CCONJ\t388\n",
            "for_ADP\t368\n",
            "at_ADP\t344\n",
            "say_VERB\t316\n",
            "very_ADV\t294\n",
            "so_ADV\t278\n",
            "on_ADP\t266\n",
            "that_SCONJ\t262\n",
            "one_NUM\t258\n",
            "as_SCONJ\t256\n",
            "have_VERB\t248\n",
            "when_SCONJ\t246\n",
            "do_AUX\t242\n",
            "see_VERB\t224\n",
            "that_PRON\t216\n",
            "out_ADP\t212\n",
            "up_ADP\t204\n",
            "make_VERB\t194\n",
            "Fox_PROPN\t192\n",
            "Wolf_PROPN\t190\n",
            "'s_PART\t178\n",
            "your_PRON\t168\n",
            "get_VERB\t168\n",
            "from_ADP\t166\n",
            "who_PRON\t166\n",
            "would_AUX\t160\n",
            "what_PRON\t156\n",
            "could_AUX\t154\n",
            "she_PRON\t154\n",
            "their_PRON\t150\n",
            "come_VERB\t150\n",
            "day_NOUN\t148\n",
            "an_DET\t148\n",
            "go_VERB\t144\n",
            "we_PRON\t144\n",
            "by_ADP\t144\n",
            "then_ADV\t140\n",
            "will_AUX\t138\n",
            "do_VERB\t138\n",
            "if_SCONJ\t134\n",
            "Lion_PROPN\t134\n",
            "all_DET\t132\n",
            "into_ADP\t132\n",
            "time_NOUN\t126\n",
            "Ass_PROPN\t126\n",
            "find_VERB\t124\n",
            "away_ADV\t122\n",
            "can_AUX\t120\n",
            "take_VERB\t116\n",
            "good_ADJ\t114\n",
            "soon_ADV\t110\n",
            "Mouse_PROPN\t110\n",
            "know_VERB\t108\n",
            "great_ADJ\t106\n",
            "now_ADV\t106\n",
            "how_SCONJ\t102\n",
            "think_VERB\t100\n",
            "her_PRON\t100\n",
            "my_PRON\t100\n",
            "about_ADP\t98\n",
            "himself_PRON\t96\n",
            "eat_VERB\t96\n",
            "down_ADP\t90\n",
            "Dog_PROPN\t90\n",
            "little_ADJ\t88\n",
            "just_ADV\t88\n",
            "ask_VERB\t88\n",
            "head_NOUN\t86\n",
            "run_VERB\t86\n",
            "look_VERB\t86\n",
            "there_ADV\t82\n",
            "all_PRON\t82\n",
            "there_PRON\t80\n",
            "let_VERB\t80\n",
            "that_DET\t80\n",
            "some_DET\t80\n",
            "no_DET\t78\n",
            "off_ADP\t78\n",
            "as_ADV\t76\n",
            "tell_VERB\t76\n",
            "old_ADJ\t76\n",
            "as_ADP\t72\n",
            "try_VERB\t70\n",
            "fly_VERB\t70\n",
            "other_ADJ\t70\n",
            "this_DET\t70\n",
            "much_ADV\t70\n",
            "carry_VERB\t68\n",
            "should_AUX\t68\n",
            "once_ADV\t66\n",
            "keep_VERB\t66\n",
            "way_NOUN\t66\n",
            "well_ADV\t66\n",
            "be_VERB\t64\n",
            "water_NOUN\t64\n",
            "where_SCONJ\t62\n",
            "walk_VERB\t62\n",
            "much_ADJ\t62\n",
            "hear_VERB\t62\n",
            "begin_VERB\t60\n",
            "than_ADP\t60\n",
            "last_ADJ\t60\n",
            "tree_NOUN\t58\n",
            "fall_VERB\t58\n",
            "give_VERB\t58\n",
            "leave_VERB\t56\n",
            "young_ADJ\t56\n",
            "man_NOUN\t56\n",
            "call_VERB\t54\n",
            "which_PRON\t54\n",
            "__PRON\t52\n",
            "Eagle_PROPN\t52\n",
            "again_ADV\t52\n",
            "catch_VERB\t52\n",
            "far_ADV\t50\n",
            "two_NUM\t50\n",
            "cry_VERB\t50\n",
            "Cat_PROPN\t50\n",
            "live_VERB\t50\n",
            "lose_VERB\t48\n",
            "reply_VERB\t48\n",
            "may_AUX\t48\n",
            "while_SCONJ\t48\n",
            "long_ADJ\t46\n",
            "this_PRON\t46\n",
            "why_SCONJ\t46\n",
            "only_ADV\t46\n",
            "friend_NOUN\t46\n",
            "own_ADJ\t46\n",
            "among_ADP\t46\n",
            "Master_PROPN\t46\n",
            "for_SCONJ\t44\n",
            "Shepherd_PROPN\t44\n",
            "home_NOUN\t44\n",
            "even_ADV\t44\n",
            "many_ADJ\t44\n",
            "bird_NOUN\t44\n",
            "along_ADP\t44\n",
            "help_VERB\t44\n",
            "thing_NOUN\t44\n",
            "always_ADV\t42\n",
            "or_CCONJ\t42\n",
            "FOX_PROPN\t42\n",
            "without_ADP\t42\n",
            "any_DET\t42\n",
            "poor_ADJ\t42\n",
            "Monkey_PROPN\t42\n",
            "wolf_NOUN\t40\n",
            "horn_NOUN\t40\n",
            "each_DET\t40\n",
            "more_ADJ\t40\n",
            "a_PRON\t40\n",
            "stand_VERB\t40\n",
            "such_ADJ\t40\n",
            "happen_VERB\t40\n",
            "Farmer_PROPN\t40\n",
            "drive_VERB\t40\n",
            "through_ADP\t40\n",
            "fine_ADJ\t40\n",
            "Sheep_PROPN\t40\n",
            "Goat_PROPN\t38\n",
            "all_ADV\t38\n",
            "over_ADP\t38\n",
            "field_NOUN\t38\n",
            "near_ADP\t38\n",
            "turn_VERB\t38\n",
            "feel_VERB\t38\n",
            "down_ADV\t38\n",
            "one_NOUN\t38\n",
            "our_PRON\t38\n",
            "enough_ADV\t38\n",
            "after_ADP\t36\n",
            "like_ADP\t36\n",
            "__NOUN\t36\n",
            "foot_NOUN\t36\n",
            "wood_NOUN\t36\n",
            "hungry_ADJ\t36\n",
            "claw_NOUN\t36\n",
            "feather_NOUN\t36\n",
            "forest_NOUN\t36\n",
            "nothing_PRON\t36\n",
            "want_VERB\t34\n",
            "world_NOUN\t34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ’¡ Now, let's save tagged data as a dataframe and get word clouds!"
      ],
      "metadata": {
        "id": "1DoJZCxobIrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ“Œ  Tagged data is in a dictionary format (e.g., {key:value}).\n",
        "type(tagged_freq)"
      ],
      "metadata": {
        "id": "YDjwNH2xbh6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8d3fa4e-e167-4468-f079-4fbd0dd709af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ“Œ Import the pandas package so as to handle dataframe.\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "OvJt1jQBb6KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ“Œ Generate a dateframe with tagged words (e.g., word_POS) and their frequencies.\n",
        "\n",
        "data_dict = tagged_freq\n",
        "data_items = data_dict.items()\n",
        "data_list = list(data_items)\n",
        "df = pd.DataFrame(data_list)\n",
        "df.columns = [\"Tagged\",\"Freq\"]\n",
        "print(df)"
      ],
      "metadata": {
        "id": "bpGw92j8cRx6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3440ffab-ec46-47d1-d5a2-1c566a9fc859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Tagged  Freq\n",
            "0        the_DET    13\n",
            "1     Ã†sop_PROPN     1\n",
            "2        for_ADP     4\n",
            "3     child_NOUN     1\n",
            "4      Fox_PROPN     8\n",
            "..           ...   ...\n",
            "156  advice_NOUN     1\n",
            "157    seek_VERB     1\n",
            "158   lower_VERB     1\n",
            "159      own_ADJ     1\n",
            "160   level_NOUN     1\n",
            "\n",
            "[161 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ’¡ Splitting tagged columns into Words and POS <font size = '2.3'> part of speech (i.e., grammatical categories)\n",
        "  - e.g.,\n",
        "              column          column 1.    column2\n",
        "          yesterday_ADP â¡ï¸   yesterday       ADP\n",
        "          rain_NOUN             rain         NOUN\n",
        "          yellow_ADJ           yellow.       ADJ"
      ],
      "metadata": {
        "id": "riajmUc-cfJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ“Œ Codelines to get tagged columns split into words and POS\n",
        "\n",
        "tagged = df[\"Tagged\"]\n",
        "pos = []\n",
        "word = []\n",
        "\n",
        "for i in range(0, len(tagged)):\n",
        "  w = tagged[i]\n",
        "  ws = w.split(\"_\")\n",
        "  word.append(ws[0])\n",
        "  pos.append(ws[1])\n",
        "\n",
        "print(len(tagged))\n",
        "print(word[:10])\n",
        "print(pos[:10])"
      ],
      "metadata": {
        "id": "_Is6HP1udiNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef767b63-4d76-4b02-c762-4f36c8b107c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "161\n",
            "['the', 'Ã†sop', 'for', 'child', 'Fox', 'without', 'a', 'Tail', 'that', 'have']\n",
            "['DET', 'PROPN', 'ADP', 'NOUN', 'PROPN', 'ADP', 'DET', 'PROPN', 'PRON', 'AUX']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ğŸ“Œ Add new columns to the dataframe.\n",
        "\n",
        "df[\"POS\"] = pos\n",
        "df[\"Word\"] = word\n",
        "\n",
        "# Rearranging column order (remove Tagged column)\n",
        "cols = [\"POS\",\"Word\",\"Freq\"]\n",
        "df = df[cols]\n",
        "\n",
        "# Sort by POS and Freq\n",
        "df = df.sort_values(by=['POS', 'Freq'], ascending = False)\n",
        "print(\"Total rows: \", len(df))\n",
        "df.head()"
      ],
      "metadata": {
        "id": "xVE9PFHvfBdm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "dbcce4c8-9bac-4017-b330-5b9d36506a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows:  161\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     POS   Word  Freq\n",
              "25  VERB   have     4\n",
              "69  VERB    say     4\n",
              "11  VERB  catch     2\n",
              "21  VERB    get     2\n",
              "40  VERB   know     2"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5b4784e6-f8cc-4009-82f3-ecb84aa5326d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>POS</th>\n",
              "      <th>Word</th>\n",
              "      <th>Freq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>VERB</td>\n",
              "      <td>have</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>VERB</td>\n",
              "      <td>say</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>VERB</td>\n",
              "      <td>catch</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>VERB</td>\n",
              "      <td>get</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>VERB</td>\n",
              "      <td>know</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b4784e6-f8cc-4009-82f3-ecb84aa5326d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5b4784e6-f8cc-4009-82f3-ecb84aa5326d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5b4784e6-f8cc-4009-82f3-ecb84aa5326d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}